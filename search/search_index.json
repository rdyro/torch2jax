{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>torch2jax</code>","text":"<p>This package is designed to facilitate no-copy PyTorch calling from JAX under both eager execution and JIT. It leverages the JAX C++ extension interface, enabling operations on both CPU and GPU platforms. Moreover, it allows for executing arbitrary PyTorch code from JAX under eager execution and JIT.</p> <p>The intended application is efficiently running existing PyTorch code (like ML models) in JAX applications with very low overhead.</p> <p>This project was inspired by the jax2torch repository https://github.com/lucidrains/jax2torch and has been made possible due to an amazing tutorial on extending JAX https://github.com/dfm/extending-jax. Comprehensive JAX documentation https://github.com/google/jax also significantly contributed to this work.</p> <p>Although I am unsure this functionality could be achieved without C++/CUDA, the C++ compilation is efficiently done using PyTorch's portable CUDA &amp; C++ compilation features, requiring minimal configuration.</p> <p>Tested and developed with (python==3.9.13, torch==2.0.1 jax==0.4.8)</p>"},{"location":"#install","title":"Install","text":"<pre><code>$ pip install git+https://github.com/rdyro/torch2jax.git\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>With a single output</p> <pre><code>import torch\nimport jax\nfrom jax import numpy as jnp\nfrom torch2jax import torch2jax # this converts a Python function to JAX\nfrom torch2jax import Size # this is torch.Size, a tuple-like shape representation\n\ndef torch_fn(a, b):\n      return a + b\n\nshape = (10, 2)\na, b = torch.randn(shape), torch.randn(shape)\njax_fn = torch2jax(torch_fn, a, b)  # without output_shapes, torch_fn **will be evaluated once**\njax_fn = torch2jax(torch_fn, a, b, output_shapes=Size(a.shape))  # torch_fn will NOT be evaluated\n\nprngkey = jax.random.PRNGKey(0)\ndevice = jax.devices(\"cuda\")[0] # both CPU and CUDA are supported\na = jax.device_put(jax.random.normal(prngkey, shape), device)\nb = jax.device_put(jax.random.normal(prngkey, shape), device)\n\n# call the no-copy torch function\nout = jax_fn(a, b)\n\n# call the no-copy torch function **under JIT**\nout = jax.jit(jax_fn)(a, b)\n</code></pre> <p>With a multiple outputs</p> <pre><code>def torch_fn(a, b):\n    layer = torch.nn.Linear(2, 20).to(a)\n    return a + b, torch.norm(a), layer(a * b)\n\n\nshape = (10, 2)\na, b = torch.randn(shape), torch.randn(shape)\njax_fn = torch2jax(torch_fn, a, b)  # with example argumetns\n\nprngkey = jax.random.PRNGKey(0)\ndevice = jax.devices(\"cuda\")[0]\na = jax.device_put(jax.random.normal(prngkey, shape), device)\nb = jax.device_put(jax.random.normal(prngkey, shape), device)\n\n# call the no-copy torch function\nx, y, z = jax_fn(a, b)\n\n# call the no-copy torch function **under JIT**\nx, y, z = jax.jit(jax_fn)(a, b)\n</code></pre>"},{"location":"#automatically-defining-gradients","title":"Automatically defining gradients","text":"<p>Automatic reverse-mode gradient definitions are now supported for wrapped pytorch functions with the method <code>torch2jax_with_vjp</code></p> <pre><code>import torch\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\n\nfrom torch2jax import torch2jax_with_vjp\n\ndef torch_fn(a, b):\n  return torch.nn.MSELoss()(a, b)\n\nshape = (6,)\n\nxt, yt = torch.randn(shape), torch.randn(shape)\n\n# `depth` determines how many times the function can be differentiated\njax_fn = torch2jax_with_vjp(torch_fn, xt, yt, depth=2) \n\n\n# we can now differentiate the function (derivatives are taken using PyTorch autodiff)\ng_fn = jax.grad(jax_fn, argnums=(0, 1))\nx, y = jnp.array(np.random.randn(*shape)), jnp.array(np.random.randn(*shape))\n\nprint(g_fn(x, y))\n\n# JIT works too\nprint(jax.jit(g_fn)(x, y))\n</code></pre> <p>Caveats: </p> <ul> <li><code>jax.hessian(f)</code> will not work since <code>torch2jax</code> uses forward differentiation, but   the same functionality can be achieved using <code>jax.jacobian(jax.jacobian(f))</code></li> <li>input shapes are fixed for one wrapped function and cannot change, use   <code>torch2jax_with_vjp/torch2jax</code> again if you need to alter the input shapes</li> <li>in line with JAX philosphy, PyTorch functions must be non-mutable,   torch.func has a good description   of how to convert e.g., PyTorch models, to non-mutable formulation</li> </ul>"},{"location":"#timing-comparison-vs-pure_callback","title":"Timing Comparison vs <code>pure_callback</code>","text":"<p>This package achieves a much better performance when calling PyTorch code from JAX because it does not copy its input arguments and does not move CUDA data off the GPU.</p> <p></p>"},{"location":"#current-limitations-of-torch2jax","title":"Current Limitations of <code>torch2jax</code>","text":"<ul> <li>compilation happens on module import and can take 1-2 minutes (it will be cached afterwards)</li> <li>in the Pytorch function all arguments must be tensors, all outputs must be tensors</li> <li>all arguments must be on the same device and of the same datatype, either float32 or float64</li> <li>an input/output shape (e.g. <code>output_shapes=</code> kw argument) representations (for   flexibility in input and output structure) must be wrapped in <code>torch.Size</code> or   <code>jax.ShapeDtypeStruct</code></li> <li>the current implementation does not support batching, that's on the roadmap</li> <li>the current implementation does not define the VJP rule, in current design, this has to be done in    Python</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>version 0.4.1<ul> <li>bug-fix: in <code>torch2jax_with_vjp</code>, nondiff arguments were erroneously memorized</li> </ul> </li> </ul> <ul> <li>version 0.4.0<ul> <li>added batching (vmap support) using <code>torch.vmap</code>, this makes <code>jax.jacobian</code> work</li> <li>robustified support for gradients</li> <li>added mixed type arguments, including support for float16, float32, float64 and integer types</li> <li>removed unnecessary torch function calls in defining gradients</li> <li>added an example of wrapping a BERT model in JAX (with weights modified from JAX), <code>examples/bert_from_jax.ipynb</code></li> </ul> </li> </ul> <ul> <li>version 0.3.0<ul> <li>added a beta-version of a new wrapping method <code>torch2jax_with_vjp</code> which allows recursively defining reverse-mode gradients for the wrapped torch function that works in JAX both normally and under JIT</li> </ul> </li> </ul> <ul> <li>version 0.2.0<ul> <li>arbitrary input and output structure is now allowed</li> <li>removed the restriction on the number of arguments or their maximum dimension</li> <li>old interface is available via <code>torch2jax.compat.torch2jax</code></li> </ul> </li> </ul> <ul> <li>version 0.1.2<ul> <li>full CPU only version support, selected via <code>torch.cuda.is_available()</code></li> <li>bug-fix: compilation should now cache properly</li> </ul> </li> </ul> <ul> <li>version 0.1.1<ul> <li>bug-fix: functions do not get overwritten, manual fn id parameter replaced with automatic id generation</li> <li>compilation caching is now better</li> </ul> </li> </ul> <ul> <li>version 0.1.0<ul> <li>first working version of the package</li> </ul> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Installation depends on <code>torch</code>, <code>jax</code> and <code>numpy</code> packages.</p> <p>To install, simply run</p> <pre><code>$ pip install git+https://github.com/rdyro/torch2jax.git\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":"<ul> <li> call PyTorch functions on JAX data without input data copy</li> <li> call PyTorch functions on JAX data without input data copy under jit</li> <li> support both GPU and CPU</li> <li> (feature) support partial CPU building on systems without CUDA</li> <li> (user-friendly) support functions with a single output (return a single output, not a tuple)</li> <li> (user-friendly) support arbitrary argument input and output structure (use pytrees on the        Python side)</li> <li> (feature) support batching (e.g., support for <code>jax.vmap</code>)</li> <li> (feature) support integer input/output types</li> <li> (feature) support mixed-precision arguments in inputs/outputs</li> <li> (feature) support defining VJP for the wrapped function (import the experimental functionality        from jit-JAXFriendlyInterface)</li> <li> (tests) test how well device mapping works on multiple GPUs</li> <li> (tests) setup automatic tests for multiple versions of Python, PyTorch and JAX</li> <li> (feature) look into supporting in-place functions (support for output without copy)</li> <li> (feature) support TPU</li> </ul>"},{"location":"api/torch2jax/","title":"Converting a PyTorch Function to JAX (without gradients)","text":""},{"location":"api/torch2jax/#torch2jax","title":"<code>torch2jax</code>","text":"<code>torch2jax.api.torch2jax(fn, *example_args, example_kw=None, example_kwargs=None, output_shapes=None, input_struct=None, use_torch_vmap=True)</code> <p>Define a jit-compatible JAX function that calls a PyTorch function.  Arbitrary nesting of arguments and outputs is supported.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>PyTorch function to wrap.</p> required <code>*example_args</code> <code>Any</code> <p>Example arguments as tensors or torch-compatible args.</p> <code>()</code> <code>example_kw</code> <code>Any | None</code> <p>Example keyword arguments. Defaults to None.</p> <code>None</code> <code>example_kwargs</code> <code>Any | None</code> <p>Example keyword arguments. Defaults to None.</p> <code>None</code> <code>output_shapes</code> <code>Any</code> <p>Output shapes or shapes + dtype struct. Defaults to None.</p> <code>None</code> <code>input_struct</code> <code>PyTreeDef | None</code> <p>Input structure, which can be inferred from                                        example arguments and keywords. Defaults to None.</p> <code>None</code> <code>use_torch_vmap</code> <code>bool</code> <p>Whether to batch using torch.vmap or a dumb loop. Defaults to                              True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>JIT-compatible JAX function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch, jax\n&gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n&gt;&gt;&gt; # let's define the torch function and create some example arguments\n&gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n&gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n&gt;&gt;&gt; # we can not convert the function to jax using the torch fn and example args\n&gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n&gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n&gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n&gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n&gt;&gt;&gt; jax_fn(x, y)\n&gt;&gt;&gt; # it works!\n</code></pre> Source code in <code>torch2jax/api.py</code> <pre><code>def torch2jax(\n    fn: Callable,\n    *example_args: Any,\n    example_kw: Any | None = None,\n    example_kwargs: Any | None = None,\n    output_shapes: Any = None,\n    input_struct: PyTreeDef | None = None,\n    use_torch_vmap: bool = True,\n) -&gt; Callable:\n\"\"\"Define a jit-compatible JAX function that calls a PyTorch function.  Arbitrary nesting of\n    arguments and outputs is supported.\n\n    Args:\n        fn (Callable): PyTorch function to wrap.\n        *example_args (Any): Example arguments as tensors or torch-compatible args.\n        example_kw (Any | None, optional): Example keyword arguments. Defaults to None.\n        example_kwargs (Any | None, optional): Example keyword arguments. Defaults to None.\n        output_shapes (Any, optional): Output shapes or shapes + dtype struct. Defaults to None.\n        input_struct (PyTreeDef | None, optional): Input structure, which can be inferred from\n                                                   example arguments and keywords. Defaults to None.\n        use_torch_vmap (bool, optional): Whether to batch using torch.vmap or a dumb loop. Defaults to\n                                         True.\n    Returns:\n        Callable: JIT-compatible JAX function.\n\n    Examples:\n        &gt;&gt;&gt; import torch, jax\n        &gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n        &gt;&gt;&gt; # let's define the torch function and create some example arguments\n        &gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n        &gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n        &gt;&gt;&gt; # we can not convert the function to jax using the torch fn and example args\n        &gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n        &gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n        &gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n        &gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n        &gt;&gt;&gt; jax_fn(x, y)\n        &gt;&gt;&gt; # it works!\n    \"\"\"\n\n    # check for presence of example_args and example_kw\n    msg = \"Please provide either example_kw or example_kwargs, not both.\"\n    assert example_kw is None or example_kwargs is None, msg\n    if example_kwargs is not None:\n        example_kw = example_kwargs\n    has_kw = example_kw is not None\n\n    if input_struct is None:\n        if has_kw:\n            input_struct = tree_structure((example_args, example_kw))\n        else:\n            input_struct = tree_structure(example_args)\n\n    if output_shapes is None:\n        with torch.no_grad():\n            output = fn(*example_args, **example_kw) if has_kw else fn(*example_args)\n        output_shapes, output_struct = tree_flatten(\n            tree_map(lambda x: ShapeDtypeStruct(x.shape, dtype_t2j(x.dtype)), output)\n        )\n\n    else:\n        output_shapes, output_struct = tree_flatten(output_shapes)\n        msg = \"Please provide all shapes as torch.Size or jax.ShapeDtypeStruct.\"\n        assert all(\n            isinstance(x, (torch.Size, ShapedArray, ShapeDtypeStruct)) or hasattr(x, \"shape\")\n            for x in output_shapes\n        ), msg\n\n    # define flattened version of the function (flat arguments and outputs)\n    def flat_fn(*args_flat):\n        nonlocal output_shapes, example_args\n        if has_kw:\n            args, kw = tree_unflatten(input_struct, args_flat)\n            ret = fn(*args, **kw)\n        else:\n            args = tree_unflatten(input_struct, args_flat)\n            ret = fn(*args)\n        return tree_flatten(ret)[0]\n\n    # define the wrapped function using flat interface\n    wrapped_fn_flat = torch2jax_flat(\n        flat_fn, output_shapes=output_shapes, use_torch_vmap=use_torch_vmap\n    )\n\n    if has_kw:\n\n        def wrapped_fn(*args, **kw):\n            ret = wrapped_fn_flat(*tree_flatten((args, kw))[0])\n            return tree_unflatten(output_struct, ret)\n\n    else:\n\n        def wrapped_fn(*args):\n            ret = wrapped_fn_flat(*tree_flatten(args)[0])\n            return tree_unflatten(output_struct, ret)\n\n    return wrapped_fn\n</code></pre>"},{"location":"api/torch2jax_with_vjp/","title":"Defining gradients automatically: support for AutoDiff","text":""},{"location":"api/torch2jax_with_vjp/#torch2jax_with_vjp","title":"<code>torch2jax_with_vjp</code>","text":"<code>torch2jax.gradients.torch2jax_with_vjp(torch_fn, *example_args, depth=2, nondiff_argnums=None, nondiff_mask=None, output_shapes=None, use_zeros=True, use_torch_vjp=True, use_torch_vmap=None)</code> <p>Convert a torch function to a jax function and define a custom vjp rule for it up to <code>depth</code> recursively deep.</p> <p>Parameters:</p> Name Type Description Default <code>torch_fn</code> <code>Callable</code> <p>Torch function to convert.</p> required <code>*example_args</code> <code>Any</code> <p>Example arguments as tensors or torch-compatible args.</p> <code>()</code> <code>depth</code> <code>int</code> <p>Max allowed differentiation depth, this is cheap. Defaults to 1.</p> <code>2</code> <code>nondiff_argnums</code> <code>list | tuple | None</code> <p>Which (whole) args to                                              not differentiate. Defaults to None.</p> <code>None</code> <code>nondiff_mask</code> <code>Any | None</code> <p>Full arg matching mask. Defaults to None.</p> <code>None</code> <code>output_shapes</code> <code>Any | None</code> <p>Output shapes out of the function,                                   if provided, we never call torch                                   function to infer them. Defaults                                   to None.</p> <code>None</code> <code>use_zeros</code> <code>bool</code> <p>Whether to set gradients of non-diff args to                         zeros or None. None does not appear to work                         with JAX currently. Defaults to True.</p> <code>True</code> <code>use_torch_vjp</code> <code>bool</code> <p>Whether to use custom vjp or the one from torch. False means                             fallback to <code>torch.autograd.grad</code> for more compatibility.                             Some older external library PyTorch code may need this                             fallback. Defaults to True (i.e., do not use fallback).</p> <code>True</code> <code>use_torch_vmap</code> <code>bool | None</code> <p>Whether to torch.vmap for batch or a dumb for loop.                                     Defaults to True for use_torch_vjp and False                                     otherwise.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>JIT-compatible JAX version of the torch function (VJP defined up to depth <code>depth</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch, jax\n&gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n&gt;&gt;&gt; # let's define the torch function and create some example arguments\n&gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n&gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n&gt;&gt;&gt; # we can not convert the function to jax using the torch fn and example args\n&gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n&gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n&gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n&gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n&gt;&gt;&gt; jax_fn(x, y)\n&gt;&gt;&gt; # it works!\n</code></pre> <pre><code>&gt;&gt;&gt; # taking gradients is easy too\n&gt;&gt;&gt; g_fn = jax.grad(jax_fn, argnums=0)\n&gt;&gt;&gt; g_fn(x, y).shape\n(10, 5)\n</code></pre> <pre><code>&gt;&gt;&gt; # creating a more complicated computational graph is of course possible\n&gt;&gt;&gt; lin_model = lambda z, W, b: z @ W + b\n&gt;&gt;&gt; z, W, b = tree_t2j([torch.randn((10, 20)), torch.randn(20, 5), torch.randn(5)])\n&gt;&gt;&gt; gz_fn = jax.grad(lambda z, W, b: jax_fn(lin_model(z, W, b), y), argnums=(1, 2))\n&gt;&gt;&gt; dW, db = gz_fn(z, W, b)\n&gt;&gt;&gt; dW.shape, db.shape\n((20, 5), (5,))\n</code></pre> Source code in <code>torch2jax/gradients.py</code> <pre><code>def torch2jax_with_vjp(\n    torch_fn: Callable,\n    *example_args: Any,\n    depth: int = 2,\n    nondiff_argnums: list | tuple | None = None,\n    nondiff_mask: Any | None = None,\n    output_shapes: Any | None = None,\n    use_zeros: bool = True,\n    use_torch_vjp: bool = True,\n    use_torch_vmap: bool | None = None,\n) -&gt; Callable:\n\"\"\"Convert a torch function to a jax function and define a custom vjp rule\n    for it up to `depth` recursively deep.\n\n    Args:\n        torch_fn (Callable): Torch function to convert.\n        *example_args (Any): Example arguments as tensors or torch-compatible args.\n        depth (int, optional): Max allowed differentiation depth, this is cheap. Defaults to 1.\n        nondiff_argnums (list | tuple | None, optional): Which (whole) args to\n                                                         not differentiate. Defaults to None.\n        nondiff_mask (Any | None, optional): Full arg matching mask. Defaults to None.\n        output_shapes (Any | None, optional): Output shapes out of the function,\n                                              if provided, we never call torch\n                                              function to infer them. Defaults\n                                              to None.\n        use_zeros (bool, optional): Whether to set gradients of non-diff args to\n                                    zeros or None. None does not appear to work\n                                    with JAX currently. Defaults to True.\n        use_torch_vjp (bool, optional): Whether to use custom vjp or the one from torch. False means\n                                        fallback to `torch.autograd.grad` for more compatibility.\n                                        Some older external library PyTorch code may need this\n                                        fallback. Defaults to True (i.e., do not use fallback).\n        use_torch_vmap (bool | None, optional): Whether to torch.vmap for batch or a dumb for loop.\n                                                Defaults to True for use_torch_vjp and False\n                                                otherwise.\n\n    Returns:\n        Callable: JIT-compatible JAX version of the torch function (VJP defined up to depth `depth`).\n\n\n    Examples:\n        &gt;&gt;&gt; import torch, jax\n        &gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n        &gt;&gt;&gt; # let's define the torch function and create some example arguments\n        &gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n        &gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n        &gt;&gt;&gt; # we can not convert the function to jax using the torch fn and example args\n        &gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n        &gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n        &gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n        &gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n        &gt;&gt;&gt; jax_fn(x, y)\n        &gt;&gt;&gt; # it works!\n\n        &gt;&gt;&gt; # taking gradients is easy too\n        &gt;&gt;&gt; g_fn = jax.grad(jax_fn, argnums=0)\n        &gt;&gt;&gt; g_fn(x, y).shape\n        (10, 5)\n\n        &gt;&gt;&gt; # creating a more complicated computational graph is of course possible\n        &gt;&gt;&gt; lin_model = lambda z, W, b: z @ W + b\n        &gt;&gt;&gt; z, W, b = tree_t2j([torch.randn((10, 20)), torch.randn(20, 5), torch.randn(5)])\n        &gt;&gt;&gt; gz_fn = jax.grad(lambda z, W, b: jax_fn(lin_model(z, W, b), y), argnums=(1, 2))\n        &gt;&gt;&gt; dW, db = gz_fn(z, W, b)\n        &gt;&gt;&gt; dW.shape, db.shape\n        ((20, 5), (5,))\n    \"\"\"\n\n    use_torch_vmap = use_torch_vjp if use_torch_vmap is None else use_torch_vmap\n\n    if output_shapes is None:\n        with torch.no_grad():\n            outputs = torch_fn(*example_args)\n        output_shapes = tree_map(\n            lambda x: ShapeDtypeStruct(dtype=dtype_t2j(x.dtype), shape=x.shape), outputs\n        )\n    fn = torch2jax(\n        torch_fn, *example_args, output_shapes=output_shapes, use_torch_vmap=use_torch_vmap\n    )\n\n    # if this we've reached the requested differentiation depth, refrain from defining a vjp rule ##\n    if depth &lt;= 0:\n        return fn\n\n    # begin defining custom vjp ####################################################################\n    fn = jax.custom_vjp(fn)\n    example_args_flat, args_struct = tree_flatten(example_args)\n\n    # define forward function\n    def fwd_fn(*args):\n        return fn(*args), args\n\n    # handle determining which arguments are nondifferentiable #####################################\n    if nondiff_argnums is not None:\n        # assume the user means the entire e.g., 2nd arg if they pass argnums=(2,)\n        nondiff_argnums = (\n            (nondiff_argnums,) if isinstance(nondiff_argnums, int) else tuple(nondiff_argnums)\n        )\n        nondiff_mask = [\n            tree_map(lambda _: True, arg)\n            if (i in nondiff_argnums)\n            else tree_map(lambda _: False, arg)\n            for (i, arg) in enumerate(example_args)\n        ]\n    if nondiff_mask is not None:\n        nondiff_mask_flat = tree_flatten(nondiff_mask)[0]\n        assert len(nondiff_mask_flat) == len(example_args_flat), \"`nondiff_mask` must match `args`\"\n        nondiff_mask_flat = [\n            (m or (not _is_floating_point(arg)))\n            for m, arg in zip(nondiff_mask_flat, example_args_flat)\n        ]\n    else:\n        nondiff_mask_flat = [not _is_floating_point(arg) for i, arg in enumerate(example_args_flat)]\n\n    # define two torch helper functions for computing the VJP ######################################\n    def _torch_fn_diff_flat(*diff_args_flat, all_args_flat=None):\n        args_collected_flat, diff_args_flat = [], list(diff_args_flat)\n        for arg, m in zip(all_args_flat, nondiff_mask_flat):\n            args_collected_flat.append(arg if m else diff_args_flat.pop(0))\n        args_collected = tree_unflatten(args_struct, args_collected_flat)\n        return tree_flatten(torch_fn(*args_collected))[0]\n\n    # define the actual torch VJP function #########################################################\n    def bwd_fn_torch(args, gs):\n        args_flat = tree_flatten(args)[0]\n        diff_args_flat = [arg for (arg, m) in zip(args_flat, nondiff_mask_flat) if not m]\n        gs_flat = tree_flatten(gs)[0]\n\n        # use either torch's vjp or our custom vjp only wrt differentiable arguments ###############\n        grads_computed = False\n        if use_torch_vjp:\n            try:\n                diff_vjp_vals_flat = list(\n                    torch.func.vjp(\n                        partial(_torch_fn_diff_flat, all_args_flat=args_flat), *diff_args_flat\n                    )[1](gs_flat)\n                )\n                grads_computed = True\n            except RuntimeError:\n                tb = traceback.format_exc()\n                msg = (\n                    \"Somewhere in your PyTorch computation graph, a custom backward function is \"\n                    + \"defined in the old way (see \"\n                    + \"https://pytorch.org/docs/stable/notes/extending.html). This is only \"\n                    + \" experimentally supported in torch2jax. We will use a fallback based on \"\n                    + \"`torch.autograd.grad` instead. Please pass `use_torch_vjp=False` to \"\n                    + \" `torch2jax_with_vjp` if you wish to use this fallback explicitly.\"\n                )\n                msg = \"\\n\".join([\"#\" * 80, msg, tb, \"#\" * 80])\n                warn(msg)\n                grads_computed = False\n        if not grads_computed:\n            if not use_torch_vjp:\n                warn(\"You are NOT using PyTorch's functional VJP. This is highly experimental.\")\n            [diff_arg_flat.requires_grad_(True) for diff_arg_flat in diff_args_flat]\n            ret = sum(\n                torch.sum(g * r)\n                for (g, r) in zip(\n                    gs_flat, _torch_fn_diff_flat(*diff_args_flat, all_args_flat=args_flat)\n                )\n            )\n            diff_vjp_vals_flat = list(torch.autograd.grad(ret, diff_args_flat, create_graph=True))\n\n        # reconstruct the full vjp including for nondiff arguments #################################\n        vjp_vals_flat = []\n        for arg, m in zip(args_flat, nondiff_mask_flat):\n            vjp_vals_flat.append(\n                (None if not use_zeros else 0 * arg) if m else diff_vjp_vals_flat.pop(0)\n            )\n        return tree_unflatten(args_struct, vjp_vals_flat)\n\n    # construct example outputs out of the bwd_fn (sensitivty wrt args) ############################\n    # and next shapes (args, outputs) ##############################################################\n    example_outputs = normalize_shapes(output_shapes, example_args)\n    next_output_shapes = tree_unflatten(\n        args_struct,\n        [\n            ShapeDtypeStruct(dtype=dtype_t2j(x.dtype), shape=x.shape)\n            if (not m or use_zeros)\n            else None\n            for (x, m) in zip(example_args_flat, nondiff_mask_flat)\n        ],\n    )\n    bwd_fn = torch2jax_with_vjp(\n        bwd_fn_torch,\n        example_args,\n        example_outputs,\n        output_shapes=next_output_shapes,\n        depth=depth - 1,\n        use_torch_vjp=use_torch_vjp,\n        use_torch_vmap=use_torch_vmap,\n    )\n    # define the custom vjp using the fwd_fn and bwd_fn ############################################\n    fn.defvjp(fwd_fn, bwd_fn)\n\n    return fn\n</code></pre>"},{"location":"examples/bert_example/","title":"Calling BERT model from JAX (with BERT weights in JAX)","text":"<pre><code>from __future__ import annotations\n\nimport sys\nfrom pathlib import Path\nimport random\nimport time\n\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel\nfrom datasets import load_dataset\nimport torch\nfrom torch import Tensor\nfrom torch.func import functional_call\nimport jax\nfrom jax import numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torch2jax import tree_t2j, torch2jax_with_vjp\n</code></pre>"},{"location":"examples/bert_example/#loading-the-dataset-and-the-model-in-pytorch","title":"Loading the dataset and the model (in PyTorch)","text":"<pre><code>dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\nmodel.to(device)\nmodel.eval()\n\n\ndef tokenizer_torch(text: list[str]) -&gt; dict[str, Tensor]:\n    encoded = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n    return {k: v.to(device) for (k, v) in encoded.items()}\n</code></pre>"},{"location":"examples/bert_example/#lets-convert-the-torch-model-to-a-function-using-torchfuncfunctional_call","title":"Let's convert the torch model to a function, using <code>torch.func.functional_call</code>","text":"<pre><code>params, buffers = dict(model.named_parameters()), dict(model.named_buffers())\n\ndef torch_fwd_fn(params, buffers, input):\n    return functional_call(model, (params, buffers), args=(), kwargs=input).pooler_output\n\nnb = 16\ntext = [x[\"text\"] for x in random.choices(dataset, k=int(1e3)) if len(x[\"text\"]) &gt; 100][:nb]\nencoded_text = tokenizer_torch(text)\n</code></pre>"},{"location":"examples/bert_example/#we-do-not-need-to-specify-output-the-library-will-call-the-torch-function-ones-to-infer-the-output","title":"We do not need to specify output, the library will call the torch function ones to infer the output","text":"<pre><code>jax_fwd_fn = jax.jit(torch2jax_with_vjp(torch_fwd_fn, params, buffers, encoded_text))\nparams_jax, buffers_jax = tree_t2j(params), tree_t2j(buffers)\nencoded_text_jax = tree_t2j(encoded_text)\n</code></pre>"},{"location":"examples/bert_example/#taking-gradients-wrt-model-parameters","title":"Taking gradients wrt model parameters","text":"<pre><code>g_fn = jax.jit(jax.grad(lambda params: jnp.sum(jax_fwd_fn(params, buffers_jax, encoded_text_jax))))\ng_torch_fn = torch.func.grad(lambda params: torch.sum(torch_fwd_fn(params, buffers, encoded_text)))\ngs = g_fn(params_jax)\ngs_torch = tree_t2j(g_torch_fn(params))\n\n# let's compare the errors in gradients (they will  be 0!)\ntotal_err = 0\nfor k in gs.keys():\n    err = jnp.linalg.norm(gs[k] - gs_torch[k])\n    total_err += err\nprint(f\"Total error in gradient: {total_err:.4e}\")\n</code></pre> <p>Total error in gradient: 0.0000e+00</p>"},{"location":"examples/bert_example/#timing-the-gains-over-pure_callback","title":"Timing the gains over <code>pure_callback</code>","text":"<pre><code>root_path = Path(\"\").absolute().parent / \"tests\"\nif str(root_path) not in sys.path:\n    sys.path.append(str(root_path))\n\nfrom pure_callback_alternative import wrap_torch_fn\n</code></pre> <pre><code>with torch.no_grad():\n    output_shapes = model(**encoded_text).pooler_output\njax_fwd_fn2 = jax.jit(wrap_torch_fn(torch_fwd_fn, output_shapes, device=device.type))\n</code></pre> <pre><code>t1s, t2s, t3s = [], [], []\nfor i in tqdm(range(100)):\n    text = [x[\"text\"] for x in random.choices(dataset, k=int(1e3)) if len(x[\"text\"]) &gt; 100][:nb]\n    encoded_text = tokenizer_torch(text)\n    encoded_text_jax = tree_t2j(encoded_text)\n\n    t = time.time()\n    out1 = jax_fwd_fn(params_jax, buffers_jax, encoded_text_jax)\n    out1.block_until_ready()\n    t = time.time() - t\n    t1s.append(t)\n\n    t = time.time()\n    out2 = jax_fwd_fn2(params_jax, buffers_jax, encoded_text_jax)[0]\n    out2.block_until_ready()\n    t = time.time() - t\n    t2s.append(t)\n\n    t = time.time()\n    with torch.no_grad():\n        out3 = model(**encoded_text).pooler_output\n    torch.cuda.synchronize()\n    t = time.time() - t\n    t3s.append(t)\n</code></pre> <pre><code>plt.figure()\nplt.bar(\n    [\"jax (pure_callback)\", \"jax (torch2jax)\", \"torch\"],\n    [np.mean(t2s), np.mean(t1s), np.mean(t3s)],\n    yerr=[np.std(t2s), np.std(t1s), np.std(t3s)],\n    capsize=10.0,\n)\nplt.ylabel(\"Time (s)\")\nplt.tight_layout()\nplt.savefig(\"../images/bert_from_jax.png\", dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\nplt.show()\n</code></pre>"},{"location":"examples/resnet_example/","title":"ResNet 50 example","text":"<pre><code>from __future__ import annotations\n\nfrom pprint import pprint\n\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nimport torch\nfrom torchvision.models.resnet import resnet18\nfrom torch import nn\nfrom torch.func import functional_call\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nimport jax\nfrom jax import numpy as jnp\nimport optax\n\nfrom torch2jax import tree_t2j, torch2jax_with_vjp, tree_j2t, t2j, j2t\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice_jax = jax.devices(device.type)[0]\n</code></pre>"},{"location":"examples/resnet_example/#loading-the-dataset-and-the-model-in-pytorch","title":"Loading the dataset and the model (in PyTorch)","text":"<pre><code>dataset = load_dataset(\"mnist\", split=\"train\")\n\ndef collate_torch_fn(batch):\n    imgs = torch.stack([ToTensor()(x[\"image\"]).repeat((3, 1, 1)) for x in batch]).to(device)\n    labels = torch.tensor([x[\"label\"] for x in batch]).to(device)\n    return imgs, labels\n\ncollate_jax_fn = lambda batch: tree_t2j(collate_torch_fn(batch))\n</code></pre> <pre><code>model = nn.Sequential(resnet18(), nn.Linear(1000, 10))\nmodel.to(device)\nmodel.eval()\n\nopts = dict(batch_size=32, shuffle=True, num_workers=0)\ndl = DataLoader(dataset, **opts)\ndl_jax = DataLoader(dataset, **dict(opts, collate_fn=collate_jax_fn))\ndl_torch = DataLoader(dataset, **dict(opts, collate_fn=collate_torch_fn))\n</code></pre>"},{"location":"examples/resnet_example/#lets-convert-the-torch-model-to-a-function-using-torchfuncfunctional_call","title":"Let's convert the torch model to a function, using <code>torch.func.functional_call</code>","text":"<pre><code>params, buffers = dict(model.named_parameters()), dict(model.named_buffers())\n\n\ndef torch_fwd_fn(params, buffers, input):\n    buffers = {k: torch.clone(v) for k, v in buffers.items()}\n    return functional_call(model, (params, buffers), args=input)\n\n\nXt, yt = next(iter(dl_torch))\nnondiff_argnums = (1, 2)  # buffers, input\njax_fwd_fn = jax.jit(\n    torch2jax_with_vjp(torch_fwd_fn, params, buffers, Xt, nondiff_argnums=nondiff_argnums)\n)\nparams_jax, buffers_jax = tree_t2j(params), tree_t2j(buffers)\n</code></pre>"},{"location":"examples/resnet_example/#lets-use-torchs-crossentropyloss","title":"Let's use torch's CrossEntropyLoss","text":"<pre><code>Xt, yt = next(iter(dl_torch))\ntorch_ce_fn = lambda yp, y: nn.CrossEntropyLoss()(yp, y)\njax_ce_fn = torch2jax_with_vjp(torch_ce_fn, model(Xt), yt)\n\njax_l_fn = jax.jit(\n    lambda params_jax, X, y: jnp.mean(jax_ce_fn(jax_fwd_fn(params_jax, buffers_jax, X), y))\n)\njax_g_fn = jax.jit(jax.grad(jax_l_fn))\ntorch_g_fn = torch.func.grad(\n    lambda params, Xt, yt: torch_ce_fn(torch_fwd_fn(params, buffers, Xt), yt)\n)\n</code></pre> <pre><code>X, y = next(iter(dl_jax))\ngs_jax = jax_g_fn(params_jax, X, y)\ngs_torch = torch_g_fn(params, *tree_j2t((X, y)))\n\n# let's compute error in gradients between JAX and Torch (the errors are 0!)\nerrors = {k: float(jnp.linalg.norm(v - t2j(gs_torch[k]))) for k, v in gs_jax.items()}\npprint(errors)\n</code></pre> <p> {'0.bn1.bias': 6.606649449736324e-09,  '0.bn1.weight': 1.0237145575686668e-09,  '0.conv1.weight': 1.9232666659263487e-07,  '0.fc.bias': 0.0,  '0.fc.weight': 0.0,  '0.layer1.0.bn1.bias': 4.424356436771859e-09,  '0.layer1.0.bn1.weight': 5.933196711715993e-10,  '0.layer1.0.bn2.bias': 2.3588471176339e-09,  '0.layer1.0.bn2.weight': 4.533372566228877e-10,  '0.layer1.0.conv1.weight': 1.4028480599392879e-08,  '0.layer1.0.conv2.weight': 1.1964990775936712e-08,  '0.layer1.1.bn1.bias': 8.75052974524948e-10,  '0.layer1.1.bn1.weight': 2.0072446482721773e-10,  '0.layer1.1.bn2.bias': 5.820766091346741e-11,  '0.layer1.1.bn2.weight': 2.9103830456733704e-11,  '0.layer1.1.conv1.weight': 1.1259264631746646e-08,  '0.layer1.1.conv2.weight': 1.1262083710050774e-08,  '0.layer2.0.bn1.bias': 0.0,  '0.layer2.0.bn1.weight': 0.0,  '0.layer2.0.bn2.bias': 0.0,  '0.layer2.0.bn2.weight': 0.0,  '0.layer2.0.conv1.weight': 0.0,  '0.layer2.0.conv2.weight': 0.0,  '0.layer2.0.downsample.0.weight': 6.819701248161891e-09,  '0.layer2.0.downsample.1.bias': 0.0,  '0.layer2.0.downsample.1.weight': 0.0,  '0.layer2.1.bn1.bias': 0.0,  '0.layer2.1.bn1.weight': 0.0,  '0.layer2.1.bn2.bias': 0.0,  '0.layer2.1.bn2.weight': 5.820766091346741e-11,  '0.layer2.1.conv1.weight': 0.0,  '0.layer2.1.conv2.weight': 0.0,  '0.layer3.0.bn1.bias': 0.0,  '0.layer3.0.bn1.weight': 0.0,  '0.layer3.0.bn2.bias': 0.0,  '0.layer3.0.bn2.weight': 0.0,  '0.layer3.0.conv1.weight': 0.0,  '0.layer3.0.conv2.weight': 0.0,  '0.layer3.0.downsample.0.weight': 0.0,  '0.layer3.0.downsample.1.bias': 0.0,  '0.layer3.0.downsample.1.weight': 0.0,  '0.layer3.1.bn1.bias': 0.0,  '0.layer3.1.bn1.weight': 0.0,  '0.layer3.1.bn2.bias': 0.0,  '0.layer3.1.bn2.weight': 0.0,  '0.layer3.1.conv1.weight': 0.0,  '0.layer3.1.conv2.weight': 0.0,  '0.layer4.0.bn1.bias': 0.0,  '0.layer4.0.bn1.weight': 0.0,  '0.layer4.0.bn2.bias': 0.0,  '0.layer4.0.bn2.weight': 0.0,  '0.layer4.0.conv1.weight': 0.0,  '0.layer4.0.conv2.weight': 0.0,  '0.layer4.0.downsample.0.weight': 0.0,  '0.layer4.0.downsample.1.bias': 0.0,  '0.layer4.0.downsample.1.weight': 0.0,  '0.layer4.1.bn1.bias': 0.0,  '0.layer4.1.bn1.weight': 0.0,  '0.layer4.1.bn2.bias': 0.0,  '0.layer4.1.bn2.weight': 0.0,  '0.layer4.1.conv1.weight': 0.0,  '0.layer4.1.conv2.weight': 0.0,  '1.bias': 0.0,  '1.weight': 0.0} </p>"},{"location":"examples/resnet_example/#train-loop","title":"Train loop","text":"<p>This isn't very efficient because torch synchronizes for every batch when called from JAX. Train in PyTorch, but you can do inference in JAX fast.</p> <pre><code>optimizer = optax.adam(1e-3)\nopt_state = optimizer.init(params_jax)\nupdate_fn, apply_updates = jax.jit(optimizer.update), jax.jit(optax.apply_updates)\nfor i, (X, y) in enumerate(tqdm(dl_jax, total=len(dl_jax))):\n    gs = jax_g_fn(params_jax, X, y)\n    updates, opt_state = update_fn(gs, opt_state)\n    params_jax2 = apply_updates(params_jax, updates)\n    if i &gt; 10:\n        break\n</code></pre>"}]}