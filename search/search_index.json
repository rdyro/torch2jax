{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"torch2jax","text":"<p>Documentation</p> <p> <p> </p> <p> </p> <p>This package is designed to facilitate no-copy PyTorch calling from JAX under both eager execution and JIT. It leverages the JAX C++ extension interface, enabling operations on both CPU and GPU platforms. Moreover, it allows for executing arbitrary PyTorch code from JAX under eager execution and JIT.</p> <p>The intended application is efficiently running existing PyTorch code (like ML models) in JAX applications with very low overhead.</p> <p>This project was inspired by the jax2torch repository https://github.com/lucidrains/jax2torch and has been made possible due to an amazing tutorial on extending JAX https://github.com/dfm/extending-jax. Comprehensive JAX documentation https://github.com/google/jax also significantly contributed to this work.</p> <p>Although I am unsure this functionality could be achieved without C++/CUDA, the C++ compilation is efficiently done using PyTorch's portable CUDA &amp; C++ compilation features, requiring minimal configuration.</p>"},{"location":"#install","title":"Install","text":"<pre><code>$ pip install git+https://github.com/rdyro/torch2jax.git\n</code></pre> <p><code>torch2jax</code> is now available on PyPI under the alias <code>wrap_torch2jax</code>:</p> <pre><code>$ pip install wrap-torch2jax\n$ # then\n$ python3\n$ &gt;&gt;&gt; from wrap_torch2jax import torch2jax, torch2jax_with_vjp\n</code></pre> <p>Tested on:   - CPU: Python: <code>3.9 3.10 3.11 3.12</code> &amp; JAX Versions <code>0.4.26 0.4.27 0.4.28 0.4.29 0.4.30 0.4.31</code>   - CUDA: Python <code>3.9 3.10 3.11 3.12</code> &amp; JAX Versions <code>0.4.30 0.4.31</code></p>"},{"location":"#usage","title":"Usage","text":"<p>With a single output</p> <pre><code>import torch\nimport jax\nfrom jax import numpy as jnp\nfrom torch2jax import torch2jax  # this converts a Python function to JAX\nfrom torch2jax import Size, dtype_t2j  # this is torch.Size, a tuple-like shape representation\n\n\ndef torch_fn(a, b):\n    return a + b\n\n\nshape = (10, 2)\na, b = torch.randn(shape), torch.randn(shape)\njax_fn = torch2jax(torch_fn, a, b)  # without output_shapes, torch_fn **will be evaluated once**\njax_fn = torch2jax(torch_fn, a, b, output_shapes=Size(a.shape))  # torch_fn will NOT be evaluated\n\n# you can specify the whole input and output structure without instantiating the tensors\n# torch_fn will NOT be evaluated\njax_fn = torch2jax(\n    torch_fn,\n    jax.ShapeDtypeStruct(a.shape, dtype_t2j(a.dtype)),\n    jax.ShapeDtypeStruct(b.shape, dtype_t2j(b.dtype)),\n    output_shapes=jax.ShapeDtypeStruct(a.shape, dtype_t2j(a.dtype)),\n)\n\nprngkey = jax.random.PRNGKey(0)\ndevice = jax.devices(\"cuda\")[0]  # both CPU and CUDA are supported\na = jax.device_put(jax.random.normal(prngkey, shape), device)\nb = jax.device_put(jax.random.normal(prngkey, shape), device)\n\n# call the no-copy torch function\nout = jax_fn(a, b)\n\n# call the no-copy torch function **under JIT**\nout = jax.jit(jax_fn)(a, b)\n</code></pre> <p>With a multiple outputs</p> <pre><code>def torch_fn(a, b):\n    layer = torch.nn.Linear(2, 20).to(a)\n    return a + b, torch.norm(a), layer(a * b)\n\n\nshape = (10, 2)\na, b = torch.randn(shape), torch.randn(shape)\njax_fn = torch2jax(torch_fn, a, b)  # with example argumetns\n\nprngkey = jax.random.PRNGKey(0)\ndevice = jax.devices(\"cuda\")[0]\na = jax.device_put(jax.random.normal(prngkey, shape), device)\nb = jax.device_put(jax.random.normal(prngkey, shape), device)\n\n# call the no-copy torch function\nx, y, z = jax_fn(a, b)\n\n# call the no-copy torch function **under JIT**\nx, y, z = jax.jit(jax_fn)(a, b)\n</code></pre> <p>For a more advanced discussion on different ways of specifying input/output specification of the wrapped function, take a look at: input_output_specification.ipynb notebook in the <code>examples</code> folder.</p>"},{"location":"#automatically-defining-gradients","title":"Automatically defining gradients","text":"<p>Automatic reverse-mode gradient definitions are now supported for wrapped pytorch functions with the method <code>torch2jax_with_vjp</code></p> <pre><code>import torch\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\n\nfrom torch2jax import torch2jax_with_vjp\n\ndef torch_fn(a, b):\n  return torch.nn.MSELoss()(a, b)\n\nshape = (6,)\n\nxt, yt = torch.randn(shape), torch.randn(shape)\n\n# `depth` determines how many times the function can be differentiated\njax_fn = torch2jax_with_vjp(torch_fn, xt, yt, depth=2) \n\n\n# we can now differentiate the function (derivatives are taken using PyTorch autodiff)\ng_fn = jax.grad(jax_fn, argnums=(0, 1))\nx, y = jnp.array(np.random.randn(*shape)), jnp.array(np.random.randn(*shape))\n\nprint(g_fn(x, y))\n\n# JIT works too\nprint(jax.jit(g_fn)(x, y))\n</code></pre> <p>Caveats: </p> <ul> <li><code>jax.hessian(f)</code> will not work since <code>torch2jax</code> uses forward differentiation, but   the same functionality can be achieved using <code>jax.jacobian(jax.jacobian(f))</code></li> <li>input shapes are fixed for one wrapped function and cannot change, use   <code>torch2jax_with_vjp/torch2jax</code> again if you need to alter the input shapes</li> <li>in line with JAX philosphy, PyTorch functions must be non-mutable,   torch.func has a good description   of how to convert e.g., PyTorch models, to non-mutable formulation</li> </ul>"},{"location":"#new-performant-multi-gpu-experimental","title":"NEW: Performant multi-gpu (experimental)","text":"<p>User feedback greatly appreciated, feel free to open an issue if you have any questions or are running into issues: https://github.com/rdyro/torch2jax/issues/new.</p> <p><code>torch2jax</code> should now support efficient multi-gpu calling. JAX, broadly, provides 3 main ways of calling multi-device code:   - <code>shard_map</code> - per-device view, the recommended way of using <code>torch2jax</code>     - supports sharded multi-GPU arrays called in parallel     - supports automatically defining gradients   - <code>jax.jit</code> - the jit function supports automatic computation on sharded   inputs; this is currently partially supported by providing the sharding spec    of the output   - <code>jax.pmap</code> - (DOES NOT WORK) works somewhat like <code>shard_map</code>, but with   <code>jnp.stack</code> instead of <code>jnp.concatenate</code> over devices, please use <code>shard_map</code>   instead</p> <pre><code># Data-parallel model\nimport functools\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport jax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as P, NamedSharding\nfrom torch2jax import torch2jax, torch2jax_with_vjp, tree_t2j\n\n\ndef _setattr(mod, key, delim: str = \".\"):\n    if delim not in key:\n        setattr(mod, key, None)\n    else:\n        key, key_remaining = key.split(delim, 1)\n        _setattr(getattr(mod, key), key_remaining, delim=delim)\n\n\ndef _strip_model(model):\n    for key in dict(model.named_parameters()).keys():\n        _setattr(model, key, delim=\".\")\n\n\nif __name__ == \"__main__\":\n    model = nn.Sequential(nn.Linear(1024 * 1024, 1024), nn.SiLU(), nn.Linear(1024, 16)).to(\"cuda:0\")\n    params = dict(model.named_parameters())\n    [p.requires_grad_(False) for p in params.values()]\n    _strip_model(model)  # remove params from the model, leaving only a skeleton\n\n    def call_model_torch(x, params):\n        ys = []\n        for _ in range(30):\n            # functional_call uses the model in-place, we need a local copy\n            local_model_skeleton = copy.deepcopy(model)\n            ys.append(torch.func.functional_call(local_model_skeleton, params, x))\n        return sum(ys)\n\n    devices = jax.devices(\"cuda\")\n    mesh = jax.make_mesh((len(devices),), P(\"x\"), devices=devices)\n    params_sharding = NamedSharding(mesh, P())  # fully replicated\n    batch_sharding = NamedSharding(mesh, P(\"x\", None))  # sharded along batch\n\n    x = jax.jit(\n        lambda: jax.random.normal(jax.random.key(0), (128, 1024 * 1024)),\n        out_shardings=batch_sharding,\n    )()\n\n    params = jax.tree.map(lambda p: jax.device_put(p, params_sharding), tree_t2j(params))\n    params_spec = jax.tree.map(lambda _: params_sharding.spec, params)\n\n    @jax.jit\n    @functools.partial(\n        shard_map,\n        mesh=mesh,\n        in_specs=(batch_sharding.spec, params_spec),\n        out_specs=batch_sharding.spec,\n        check_rep=False,\n    )\n    def fwd_fn(x, params):\n        return torch2jax_with_vjp(call_model_torch, x, params, output_shapes=x[:, :16])(x, params)\n\n    y = fwd_fn(x, params)\n\n    # OR using JIT (but without gradients)\n    fwd_fn = jax.jit(\n        torch2jax(\n            call_model_torch, x, params, output_shapes=x[:, :16], output_sharding_spec=P(\"x\", None)\n        )\n    )\n\n    y = fwd_fn(x, params)\n</code></pre> <p> <p>Fig: Overlapping torch calls on multiple devices (RTX A4000 x 4)</p> </p> <p>Note: <code>jax.vmap</code>'s semantics might indicate that it can compute on sharded arrays, it can work, but it is not recommend, and because of <code>torch2jax</code>'s implementation will likely be executed sequentially (and likely be slow).</p>"},{"location":"#dealing-with-changing-shapes","title":"Dealing with Changing Shapes","text":"<p>You can deal with changing input shapes by calling <code>torch2jax</code> (and <code>torch2jax_with_vjp</code>) in the JAX function, both under JIT and eagerly!</p> <pre><code>@jax.jit\ndef compute(a, b, c):\n    d = torch2jax_with_vjp(\n        torch_fn,\n        jax.ShapeDtypeStruct(a.shape, dtype_t2j(a.dtype)),\n        jax.ShapeDtypeStruct(b.shape, dtype_t2j(b.dtype)),\n        output_shapes=jax.ShapeDtypeStruct(a.shape, dtype_t2j(a.dtype)),\n    )(a, b)\n    return d - c\n\nprint(compute(a, b, a))\n</code></pre>"},{"location":"#timing-comparison-vs-pure_callback","title":"Timing Comparison vs <code>pure_callback</code>","text":"<p>This package achieves a much better performance when calling PyTorch code from JAX because it does not copy its input arguments and does not move CUDA data off the GPU.</p> <p></p>"},{"location":"#current-limitations-of-torch2jax","title":"Current Limitations of <code>torch2jax</code>","text":"<ul> <li>compilation happens on module import and can take 1-2 minutes (it will be cached afterwards)</li> <li>in the Pytorch function all arguments must be tensors, all outputs must be tensors</li> <li>all arguments must be on the same device and of the same datatype, either float32 or float64</li> <li>an input/output shape (e.g. <code>output_shapes=</code> kw argument) representations (for   flexibility in input and output structure) must be wrapped in <code>torch.Size</code> or   <code>jax.ShapeDtypeStruct</code></li> <li>the current implementation does not support batching, that's on the roadmap</li> <li>the current implementation does not define the VJP rule, in current design, this has to be done in    Python</li> </ul>"},{"location":"#changelog","title":"Changelog","text":"<ul> <li>version 0.6.1<ul> <li>added <code>vmap_method=</code> support for experimental pytorch-side batching support,   see https://github.com/rdyro/torch2jax/issues/28</li> </ul> </li> </ul> <ul> <li>version 0.6.0<ul> <li>proper multi-GPU support mostly with <code>shard_map</code> but also via <code>jax.jit</code> automatic sharding</li> <li><code>shard_map</code> and automatic <code>jax.jit</code> device parallelization should work, but <code>pmap</code> doesn't work</li> <li>removed (deprecated)<ul> <li>torch2jax_flat - use the more flexible torch2jax</li> </ul> </li> <li>added input shapes validation - routines</li> </ul> </li> </ul> <ul> <li>version 0.5.0<ul> <li>updating to the new JAX ffi interface</li> </ul> </li> </ul> <ul> <li>version 0.4.11<ul> <li>compilation fixes and support for newer JAX versions</li> </ul> </li> </ul> <ul> <li>version 0.4.10<ul> <li>support for multiple GPUs, currently, all arguments must and the output   must be on the same GPU (but you can call the wrapped function with   different GPUs in separate calls)</li> <li>fixed the coming depreciation in JAX deprecating <code>.device()</code> for   <code>.devices()</code></li> </ul> </li> </ul> <ul> <li>no version change<ul> <li>added helper script <code>install_package_aliased.py</code> to automatically install   the package with a different name (to avoid a name conflict)</li> </ul> </li> </ul> <ul> <li>version 0.4.7<ul> <li>support for newest JAX (0.4.17) with backwards compatibility maintained</li> <li>compilation now delegated to python version subfolders for multi-python systems</li> </ul> </li> </ul> <ul> <li>version 0.4.6<ul> <li>bug-fix: cuda stream is now synchronized before and after a torch call explicitly to   avoid reading unwritten data</li> </ul> </li> </ul> <ul> <li>version 0.4.5<ul> <li><code>torch2jax_with_vjp</code> now automatically selects <code>use_torch_vjp=False</code> if the <code>True</code> fails</li> <li>bug-fix: cuda stream is now synchronized after a torch call explicitly to   avoid reading unwritten data</li> </ul> </li> </ul> <ul> <li>version 0.4.4<ul> <li>introduced a <code>use_torch_vjp</code> (defaulting to True) flag in <code>torch2jax_with_vjp</code> which    can be set to False to use the old <code>torch.autograd.grad</code> for taking   gradients, it is the slower method, but is more compatible</li> </ul> </li> </ul> <ul> <li>version 0.4.3<ul> <li>added a note in README about specifying input/output structure without instantiating data</li> </ul> </li> </ul> <ul> <li>version 0.4.2<ul> <li>added <code>examples/input_output_specification.ipynb</code> showing how input/output structure can be specified</li> </ul> </li> </ul> <ul> <li>version 0.4.1<ul> <li>bug-fix: in <code>torch2jax_with_vjp</code>, nondiff arguments were erroneously memorized</li> </ul> </li> </ul> <ul> <li>version 0.4.0<ul> <li>added batching (vmap support) using <code>torch.vmap</code>, this makes <code>jax.jacobian</code> work</li> <li>robustified support for gradients</li> <li>added mixed type arguments, including support for float16, float32, float64 and integer types</li> <li>removed unnecessary torch function calls in defining gradients</li> <li>added an example of wrapping a BERT model in JAX (with weights modified from JAX), <code>examples/bert_from_jax.ipynb</code></li> </ul> </li> </ul> <ul> <li>version 0.3.0<ul> <li>added a beta-version of a new wrapping method <code>torch2jax_with_vjp</code> which allows recursively defining reverse-mode gradients for the wrapped torch function that works in JAX both normally and under JIT</li> </ul> </li> </ul> <ul> <li>version 0.2.0<ul> <li>arbitrary input and output structure is now allowed</li> <li>removed the restriction on the number of arguments or their maximum dimension</li> <li>old interface is available via <code>torch2jax.compat.torch2jax</code></li> </ul> </li> </ul> <ul> <li>version 0.1.2<ul> <li>full CPU only version support, selected via <code>torch.cuda.is_available()</code></li> <li>bug-fix: compilation should now cache properly</li> </ul> </li> </ul> <ul> <li>version 0.1.1<ul> <li>bug-fix: functions do not get overwritten, manual fn id parameter replaced with automatic id generation</li> <li>compilation caching is now better</li> </ul> </li> </ul> <ul> <li>version 0.1.0<ul> <li>first working version of the package</li> </ul> </li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> call PyTorch functions on JAX data without input data copy</li> <li> call PyTorch functions on JAX data without input data copy under jit</li> <li> support both GPU and CPU</li> <li> (feature) support partial CPU building on systems without CUDA</li> <li> (user-friendly) support functions with a single output (return a single output, not a tuple)</li> <li> (user-friendly) support arbitrary argument input and output structure (use pytrees on the        Python side)</li> <li> (feature) support batching (e.g., support for <code>jax.vmap</code>)</li> <li> (feature) support integer input/output types</li> <li> (feature) support mixed-precision arguments in inputs/outputs</li> <li> (feature) support defining VJP for the wrapped function (import the experimental functionality        from jit-JAXFriendlyInterface)</li> <li> (tests) test how well device mapping works on multiple GPUs</li> <li> (tests) setup automatic tests for multiple versions of Python, PyTorch and JAX</li> <li> (feature) look into supporting in-place functions (support for output without copy)</li> <li> (feature) support TPU</li> </ul>"},{"location":"#related-work","title":"Related Work","text":"<p>Our Python package wraps PyTorch code as-is (so custom code and mutating code will work!), but if you're looking for an automatic way to transcribe a supported subset of PyTorch code to JAX, take a look at https://github.com/samuela/torch2jax/tree/main.</p> <p>We realize that two packages named the same is not ideal. As we work towards a solution, here's a stop-gap solution. We offer a helper script to install the package with an alias name, installing our package using pip under a different name.</p> <ol> <li><code>$ git clone https://github.com/rdyro/torch2jax.git</code> - clone this repo</li> <li><code>$ python3 install_package_aliased.py new_name_torch2jax --install --test</code> - install and test this package under the name <code>new_name_torch2jax</code></li> <li>you can now use this package under the name <code>new_name_torch2jax</code></li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>version 0.6.1<ul> <li>added <code>vmap_method=</code> support for experimental pytorch-side batching support,   see https://github.com/rdyro/torch2jax/issues/28</li> </ul> </li> </ul> <ul> <li>version 0.6.0<ul> <li>proper multi-GPU support mostly with <code>shard_map</code> but also via <code>jax.jit</code> automatic sharding</li> <li><code>shard_map</code> and automatic <code>jax.jit</code> device parallelization should work, but <code>pmap</code> doesn't work</li> <li>removed (deprecated)<ul> <li>torch2jax_flat - use the more flexible torch2jax</li> </ul> </li> <li>added input shapes validation - routines</li> </ul> </li> </ul> <ul> <li>version 0.5.0<ul> <li>updating to the new JAX ffi interface</li> </ul> </li> </ul> <ul> <li>version 0.4.11<ul> <li>compilation fixes and support for newer JAX versions</li> </ul> </li> </ul> <ul> <li>version 0.4.10<ul> <li>support for multiple GPUs, currently, all arguments must and the output   must be on the same GPU (but you can call the wrapped function with   different GPUs in separate calls)</li> <li>fixed the coming depreciation in JAX deprecating <code>.device()</code> for   <code>.devices()</code></li> </ul> </li> </ul> <ul> <li>no version change<ul> <li>added helper script <code>install_package_aliased.py</code> to automatically install   the package with a different name (to avoid a name conflict)</li> </ul> </li> </ul> <ul> <li>version 0.4.7<ul> <li>support for newest JAX (0.4.17) with backwards compatibility maintained</li> <li>compilation now delegated to python version subfolders for multi-python systems</li> </ul> </li> </ul> <ul> <li>version 0.4.6<ul> <li>bug-fix: cuda stream is now synchronized before and after a torch call explicitly to   avoid reading unwritten data</li> </ul> </li> </ul> <ul> <li>version 0.4.5<ul> <li><code>torch2jax_with_vjp</code> now automatically selects <code>use_torch_vjp=False</code> if the <code>True</code> fails</li> <li>bug-fix: cuda stream is now synchronized after a torch call explicitly to   avoid reading unwritten data</li> </ul> </li> </ul> <ul> <li>version 0.4.4<ul> <li>introduced a <code>use_torch_vjp</code> (defaulting to True) flag in <code>torch2jax_with_vjp</code> which    can be set to False to use the old <code>torch.autograd.grad</code> for taking   gradients, it is the slower method, but is more compatible</li> </ul> </li> </ul> <ul> <li>version 0.4.3<ul> <li>added a note in README about specifying input/output structure without instantiating data</li> </ul> </li> </ul> <ul> <li>version 0.4.2<ul> <li>added <code>examples/input_output_specification.ipynb</code> showing how input/output structure can be specified</li> </ul> </li> </ul> <ul> <li>version 0.4.1<ul> <li>bug-fix: in <code>torch2jax_with_vjp</code>, nondiff arguments were erroneously memorized</li> </ul> </li> </ul> <ul> <li>version 0.4.0<ul> <li>added batching (vmap support) using <code>torch.vmap</code>, this makes <code>jax.jacobian</code> work</li> <li>robustified support for gradients</li> <li>added mixed type arguments, including support for float16, float32, float64 and integer types</li> <li>removed unnecessary torch function calls in defining gradients</li> <li>added an example of wrapping a BERT model in JAX (with weights modified from JAX), <code>examples/bert_from_jax.ipynb</code></li> </ul> </li> </ul> <ul> <li>version 0.3.0<ul> <li>added a beta-version of a new wrapping method <code>torch2jax_with_vjp</code> which allows recursively defining reverse-mode gradients for the wrapped torch function that works in JAX both normally and under JIT</li> </ul> </li> </ul> <ul> <li>version 0.2.0<ul> <li>arbitrary input and output structure is now allowed</li> <li>removed the restriction on the number of arguments or their maximum dimension</li> <li>old interface is available via <code>torch2jax.compat.torch2jax</code></li> </ul> </li> </ul> <ul> <li>version 0.1.2<ul> <li>full CPU only version support, selected via <code>torch.cuda.is_available()</code></li> <li>bug-fix: compilation should now cache properly</li> </ul> </li> </ul> <ul> <li>version 0.1.1<ul> <li>bug-fix: functions do not get overwritten, manual fn id parameter replaced with automatic id generation</li> <li>compilation caching is now better</li> </ul> </li> </ul> <ul> <li>version 0.1.0<ul> <li>first working version of the package</li> </ul> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Installation depends on <code>torch</code>, <code>jax</code> and <code>numpy</code> packages.</p> <p>To install, simply run</p> <pre><code>$ pip install git+https://github.com/rdyro/torch2jax.git\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":"<ul> <li> call PyTorch functions on JAX data without input data copy</li> <li> call PyTorch functions on JAX data without input data copy under jit</li> <li> support both GPU and CPU</li> <li> (feature) support partial CPU building on systems without CUDA</li> <li> (user-friendly) support functions with a single output (return a single output, not a tuple)</li> <li> (user-friendly) support arbitrary argument input and output structure (use pytrees on the        Python side)</li> <li> (feature) support batching (e.g., support for <code>jax.vmap</code>)</li> <li> (feature) support integer input/output types</li> <li> (feature) support mixed-precision arguments in inputs/outputs</li> <li> (feature) support defining VJP for the wrapped function (import the experimental functionality        from jit-JAXFriendlyInterface)</li> <li> (tests) test how well device mapping works on multiple GPUs</li> <li> (tests) setup automatic tests for multiple versions of Python, PyTorch and JAX</li> <li> (feature) look into supporting in-place functions (support for output without copy)</li> <li> (feature) support TPU</li> </ul>"},{"location":"api/torch2jax/","title":"Converting a PyTorch Function to JAX (without gradients)","text":""},{"location":"api/torch2jax/#torch2jax","title":"<code>torch2jax</code>","text":"<code>torch2jax.api.torch2jax(fn, *example_args, example_kw=None, output_shapes=None, output_sharding_spec=None, vmap_method='sequential')</code> <p>Define a jit-compatible JAX function that calls a PyTorch function.  Arbitrary nesting of arguments and outputs is supported.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>PyTorch function to wrap.</p> required <code>*example_args</code> <code>Any</code> <p>Example arguments as tensors or torch-compatible args.</p> <code>()</code> <code>example_kw</code> <code>Any | None</code> <p>Example keyword arguments. Defaults to None.</p> <code>None</code> <code>output_shapes</code> <code>Any</code> <p>Output shapes or shapes + dtype struct. Defaults to None.</p> <code>None</code> <code>output_sharding_spec</code> <code>PartitionSpec | None</code> <p>jax.sharding.PartitionSpec specifying the sharding spec of the output, uses input mesh.</p> <code>None</code> <code>vmap_method</code> <code>str</code> <p>batching method, see https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap</p> <p>NOTE: only vmap_method=\"sequntial\" is supported non-experimentally</p> <p>NOTE: try \"expand_dims\", \"broadcast_all\" if you want to experiment with pytorch-side batching</p> <code>'sequential'</code> <p>Returns:     Callable: JIT-compatible JAX function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch, jax\n&gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n&gt;&gt;&gt; # let's define the torch function and create some example arguments\n&gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n&gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n&gt;&gt;&gt; # we can now convert the function to jax using the torch fn and example args\n&gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n&gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n&gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n&gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n&gt;&gt;&gt; jax_fn(x, y)\n&gt;&gt;&gt; # it works!\n</code></pre> Source code in <code>torch2jax/api.py</code> <pre><code>def torch2jax(\n    fn: Callable,\n    *example_args: Any,\n    example_kw: Any | None = None,\n    output_shapes: Any = None,\n    output_sharding_spec: PartitionSpec | None = None,\n    vmap_method: str = \"sequential\",\n) -&gt; Callable:\n    \"\"\"Define a jit-compatible JAX function that calls a PyTorch function.  Arbitrary nesting of\n    arguments and outputs is supported.\n\n    Args:\n        fn (Callable): PyTorch function to wrap.\n        *example_args (Any): Example arguments as tensors or torch-compatible args.\n        example_kw: Example keyword arguments. Defaults to None.\n        output_shapes: Output shapes or shapes + dtype struct. Defaults to None.\n        output_sharding_spec: jax.sharding.PartitionSpec specifying the sharding spec of the output, uses input mesh.\n        vmap_method: batching method, see\n            [https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap](https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap)\n\n            NOTE: only vmap_method=\"sequntial\" is supported non-experimentally\n\n            NOTE: try \"expand_dims\", \"broadcast_all\" if you want to experiment with pytorch-side batching\n    Returns:\n        Callable: JIT-compatible JAX function.\n\n    Examples:\n        &gt;&gt;&gt; import torch, jax\n        &gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n        &gt;&gt;&gt; # let's define the torch function and create some example arguments\n        &gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n        &gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n        &gt;&gt;&gt; # we can now convert the function to jax using the torch fn and example args\n        &gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n        &gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n        &gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n        &gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n        &gt;&gt;&gt; jax_fn(x, y)\n        &gt;&gt;&gt; # it works!\n    \"\"\"\n\n    # check for presence of example_args and example_kw\n    has_kw = example_kw is not None\n\n    # find the input structure\n    if has_kw:\n        input_struct = jax.tree.structure((example_args, example_kw))\n    else:\n        input_struct = jax.tree.structure(example_args)\n\n    # define flattened version of the function (flat arguments and outputs)\n    def flat_fn(*args_flat):\n        if has_kw:\n            args, kw = jax.tree.unflatten(input_struct, args_flat)\n            ret = fn(*args, **kw)\n        else:\n            args = jax.tree.unflatten(input_struct, args_flat)\n            ret = fn(*args)\n        return jax.tree.leaves(ret)\n\n    example_inputs = (example_args, example_kw) if has_kw else example_args\n    input_shapes = jax.tree.map(lambda x: ShapeDtypeStruct(x.shape, dtype_t2j(x.dtype)), example_inputs)\n\n    # find the output structure\n    if output_shapes is None:\n        with torch.no_grad():\n            output = fn(*example_args, **example_kw) if has_kw else fn(*example_args)\n        output_shapes, output_struct = jax.tree.flatten(\n            jax.tree.map(lambda x: ShapeDtypeStruct(x.shape, dtype_t2j(x.dtype)), output)\n        )\n    else:\n        if not all(\n            isinstance(x, (torch.Size, ShapeDtypeStruct, jax.Array, torch.Tensor)) or hasattr(x, \"shape\")\n            for x in jax.tree.leaves(output_shapes)\n        ):\n            warn_once(\n                \"Please provide all shapes as torch.Size or jax.ShapeDtypeStruct. We'll attempt to guess all\"\n                \" containers with only integer entries are shapes (for compatibility), but this is very error-prone.\",\n                fn,\n            )\n        output_shapes = normalize_shapes(output_shapes, extra_args=input_shapes)\n        output_shapes, output_struct = jax.tree.flatten(output_shapes)\n    if output_sharding_spec is not None:\n        output_sharding_spec_flat, output_sharding_struct = jax.tree.flatten(output_sharding_spec)\n        msg = (\n            \"When providing `output_shading_spec` its structure must match the structure of `output_shapes`.\"\n            f\"\\nExpected: {output_struct}\\n Actual:   {output_sharding_struct}\"\n        )\n        assert jax.tree.structure(output_sharding_spec) == output_struct, msg\n    else:\n        output_sharding_spec_flat, output_sharding_struct = None, None\n\n    # define the wrapped function using flat interface\n    wrapped_fn_flat = _torch2jax_flat(\n        flat_fn,\n        input_shapes=None,\n        output_shapes=output_shapes,\n        output_sharding_spec=output_sharding_spec_flat,\n        vmap_method=vmap_method,\n    )\n\n    # define the actual wrapper function\n    def wrapped_fn(*args, **kw):\n        nonlocal fn, input_shapes, output_shapes\n        if not has_kw and len(kw) &gt; 0:\n            raise RuntimeError(\"Keyword arguments not expected!\")\n        if has_kw:\n            args = (args, kw)\n            mismatch_args_msg = (\n                \"Provided (args, kw) =\\n{} do not match the torch2jax function's expected input structure =\\n{}\"\n            )\n        else:\n            mismatch_args_msg = (\n                \"Provided args =\\n{} do not match the torch2jax function's expected input structure =\\n{}\"\n            )\n        if jax.tree.structure(args) != input_struct:\n            raise RuntimeError(mismatch_args_msg.format(args, input_struct))\n\n        common_mismatch_input_msg = (\n            f\"\\nActual = {args}\\nExpected = {input_shapes}\"\n            f\"\\nAre you perhaps using a JAX transformation like `shard_map`, `vmap` or `pmap`?\"\n            \" You can try defining torch2jax eagerly inside `shard_map` or defining an un-batched version for `pmap`.\"\n            \" However, torch2jax is currently NOT WORKING with `pmap`, please use `shard_map`\"\n            \" or the experimental `auto_partitioning=True`\"\n        )\n        if output_sharding_spec:\n            if not jax.tree.all(jax.tree.map(lambda x, y: getattr(x, \"ndim\", -1) == y.ndim, args, input_shapes)):\n                msg = (\n                    \"Not all inputs to your torch2jax function match the dimensions of the expected input.\"\n                    + common_mismatch_input_msg\n                )\n                raise RuntimeError(msg)\n        else:\n            if not jax.tree.all(jax.tree.map(lambda x, y: getattr(x, \"shape\", [-1]) == y.shape, args, input_shapes)):\n                msg = (\n                    \"Not all inputs to your torch2jax function match the shapes of the expected input.\"\n                    + common_mismatch_input_msg\n                )\n                raise RuntimeError(msg)\n        ret = wrapped_fn_flat(*jax.tree.leaves(args))\n        return jax.tree.unflatten(output_struct, ret)\n\n    return wrapped_fn\n</code></pre>"},{"location":"api/torch2jax_with_vjp/","title":"Defining gradients automatically: support for AutoDiff","text":""},{"location":"api/torch2jax_with_vjp/#torch2jax_with_vjp","title":"<code>torch2jax_with_vjp</code>","text":"<code>torch2jax.gradients.torch2jax_with_vjp(torch_fn, *example_args, depth=2, nondiff_argnums=None, nondiff_mask=None, output_shapes=None, use_zeros=True, use_torch_vjp=True, output_sharding_spec=None, vmap_method='sequential')</code> <p>Convert a torch function to a jax function and define a custom vjp rule for it up to <code>depth</code> recursively deep.</p> <p>Parameters:</p> Name Type Description Default <code>torch_fn</code> <code>Callable</code> <p>Torch function to convert.</p> required <code>*example_args</code> <code>Any</code> <p>Example arguments as tensors or torch-compatible args.</p> <code>()</code> <code>depth</code> <code>int</code> <p>Max allowed differentiation depth, this is cheap. Defaults to 1.</p> <code>2</code> <code>nondiff_argnums</code> <code>list | tuple | None</code> <p>Which (whole) args to not differentiate. Defaults to None.</p> <code>None</code> <code>nondiff_mask</code> <code>Any | None</code> <p>Full arg matching mask. Defaults to None.</p> <code>None</code> <code>output_shapes</code> <code>Any | None</code> <p>Output shapes out of the function, if provided, we never call torch function to infer them. Defaults to None.</p> <code>None</code> <code>use_zeros</code> <code>bool</code> <p>Whether to set gradients of non-diff args to zeros or None. None does not appear to work with JAX currently. Defaults to True.</p> <code>True</code> <code>use_torch_vjp</code> <code>bool</code> <p>(Not supported, please use inside <code>shard_map</code>) Whether to use custom vjp or the one from torch. False means fallback to <code>torch.autograd.grad</code> for more compatibility. Some older external library PyTorch code may need this fallback. Defaults to True (i.e., do not use fallback).</p> <code>True</code> <code>output_sharding_spec</code> <code>PartitionSpec | None</code> <p>(not supported) sharding spec of the output, use shard_map instead for a device-local version of this function</p> <code>None</code> <code>vmap_method</code> <code>str</code> <p>batching method, see https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap</p> <p>NOTE: only vmap_method=\"sequntial\" is supported non-experimentally</p> <p>NOTE: try \"expand_dims\", \"broadcast_all\" if you want to experiment with pytorch-side batching</p> <code>'sequential'</code> <p>Returns:     Callable: JIT-compatible JAX version of the torch function (VJP defined up to depth <code>depth</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch, jax\n&gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n&gt;&gt;&gt; # let's define the torch function and create some example arguments\n&gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n&gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n&gt;&gt;&gt; # we can now convert the function to jax using the torch fn and example args\n&gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n&gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n&gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n&gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n&gt;&gt;&gt; jax_fn(x, y)\n&gt;&gt;&gt; # it works!\n</code></pre> <pre><code>&gt;&gt;&gt; # taking gradients is easy too\n&gt;&gt;&gt; g_fn = jax.grad(jax_fn, argnums=0)\n&gt;&gt;&gt; g_fn(x, y).shape\n(10, 5)\n</code></pre> <pre><code>&gt;&gt;&gt; # creating a more complicated computational graph is of course possible\n&gt;&gt;&gt; lin_model = lambda z, W, b: z @ W + b\n&gt;&gt;&gt; z, W, b = tree_t2j([torch.randn((10, 20)), torch.randn(20, 5), torch.randn(5)])\n&gt;&gt;&gt; gz_fn = jax.grad(lambda z, W, b: jax_fn(lin_model(z, W, b), y), argnums=(1, 2))\n&gt;&gt;&gt; dW, db = gz_fn(z, W, b)\n&gt;&gt;&gt; dW.shape, db.shape\n((20, 5), (5,))\n</code></pre> Source code in <code>torch2jax/gradients.py</code> <pre><code>def torch2jax_with_vjp(\n    torch_fn: Callable,\n    *example_args: Any,\n    depth: int = 2,\n    nondiff_argnums: list | tuple | None = None,\n    nondiff_mask: Any | None = None,\n    output_shapes: Any | None = None,\n    use_zeros: bool = True,\n    use_torch_vjp: bool = True,\n    output_sharding_spec: P | None = None,\n    vmap_method: str = \"sequential\",\n) -&gt; Callable:\n    \"\"\"Convert a torch function to a jax function and define a custom vjp rule for it up to `depth` recursively deep.\n\n    Args:\n        torch_fn (Callable): Torch function to convert.\n        *example_args (Any): Example arguments as tensors or torch-compatible args.\n        depth (int, optional): Max allowed differentiation depth, this is cheap. Defaults to 1.\n        nondiff_argnums (list | tuple | None, optional): Which (whole) args to not differentiate. Defaults to None.\n        nondiff_mask (Any | None, optional): Full arg matching mask. Defaults to None.\n        output_shapes (Any | None, optional): Output shapes out of the function, if provided, we never call torch\n            function to infer them. Defaults to None.\n        use_zeros (bool, optional): Whether to set gradients of non-diff args to zeros or None. None does not appear to\n            work with JAX currently. Defaults to True.\n        use_torch_vjp (bool, optional): (Not supported, please use inside `shard_map`) Whether to use custom vjp or the\n            one from torch. False means fallback to `torch.autograd.grad` for more compatibility. Some older external\n            library PyTorch code may need this fallback. Defaults to True (i.e., do not use fallback).\n        output_sharding_spec: (not supported) sharding spec of the output, use shard_map instead for a device-local\n            version of this function\n        vmap_method: batching method, see\n            [https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap](https://docs.jax.dev/en/latest/ffi.html#batching-with-vmap)\n\n            NOTE: only vmap_method=\"sequntial\" is supported non-experimentally\n\n            NOTE: try \"expand_dims\", \"broadcast_all\" if you want to experiment with pytorch-side batching\n    Returns:\n        Callable: JIT-compatible JAX version of the torch function (VJP defined up to depth `depth`).\n\n    Examples:\n        &gt;&gt;&gt; import torch, jax\n        &gt;&gt;&gt; from torch2jax import torch2jax_with_vjp, tree_t2j\n        &gt;&gt;&gt; # let's define the torch function and create some example arguments\n        &gt;&gt;&gt; torch_fn = lambda x, y: torch.nn.CrossEntropyLoss()(x, y)\n        &gt;&gt;&gt; xt, yt = torch.randn(10, 5), torch.randint(0, 5, (10,))\n        &gt;&gt;&gt; # we can now convert the function to jax using the torch fn and example args\n        &gt;&gt;&gt; jax_fn = torch2jax_with_vjp(torch_fn, xt, yt)\n        &gt;&gt;&gt; jax_fn = jax.jit(jax_fn) # we can jit it too\n        &gt;&gt;&gt; # let's convert the arguments to JAX arrays and call the function\n        &gt;&gt;&gt; x, y = tree_t2j((xt, yt))\n        &gt;&gt;&gt; jax_fn(x, y)\n        &gt;&gt;&gt; # it works!\n\n        &gt;&gt;&gt; # taking gradients is easy too\n        &gt;&gt;&gt; g_fn = jax.grad(jax_fn, argnums=0)\n        &gt;&gt;&gt; g_fn(x, y).shape\n        (10, 5)\n\n        &gt;&gt;&gt; # creating a more complicated computational graph is of course possible\n        &gt;&gt;&gt; lin_model = lambda z, W, b: z @ W + b\n        &gt;&gt;&gt; z, W, b = tree_t2j([torch.randn((10, 20)), torch.randn(20, 5), torch.randn(5)])\n        &gt;&gt;&gt; gz_fn = jax.grad(lambda z, W, b: jax_fn(lin_model(z, W, b), y), argnums=(1, 2))\n        &gt;&gt;&gt; dW, db = gz_fn(z, W, b)\n        &gt;&gt;&gt; dW.shape, db.shape\n        ((20, 5), (5,))\n    \"\"\"\n    if output_sharding_spec is not None:\n        raise RuntimeError(\n            \"`output_sharding_spec` not supported in `torch2jax_with_vjp`, it's somewhat difficult to automatically\"\n            \" define sharding spec for automatically defined vjp functions. As a work-around, please use this function\"\n            \" inside `shard_map` without specifying `output_sharding_spec` - you don't need to specify the specs there.\"\n        )\n\n    if output_shapes is None:\n        outputs = torch_fn(*example_args)\n        output_shapes = tree_map(lambda x: ShapeDtypeStruct(dtype=dtype_t2j(x.dtype), shape=x.shape), outputs)\n    fn = torch2jax(\n        torch_fn,\n        *example_args,\n        output_shapes=output_shapes,\n        output_sharding_spec=output_sharding_spec,\n        vmap_method=vmap_method,\n    )\n\n    # if this we've reached the requested differentiation depth, refrain from defining a vjp rule ##\n    if depth &lt;= 0:\n        return fn\n\n    # begin defining custom vjp ####################################################################\n    fn = jax.custom_vjp(fn)\n    example_args_flat, args_struct = tree_flatten(example_args)\n\n    # define forward function\n    def fwd_fn(*args):\n        return fn(*args), args\n\n    # handle determining which arguments are nondifferentiable #####################################\n    if nondiff_argnums is not None:\n        # assume the user means the entire e.g., 2nd arg if they pass argnums=(2,)\n        nondiff_argnums = (nondiff_argnums,) if isinstance(nondiff_argnums, int) else tuple(nondiff_argnums)\n        nondiff_mask = [\n            tree_map(lambda _: True, arg) if (i in nondiff_argnums) else tree_map(lambda _: False, arg)\n            for (i, arg) in enumerate(example_args)\n        ]\n    if nondiff_mask is not None:\n        nondiff_mask_flat = tree_flatten(nondiff_mask)[0]\n        assert len(nondiff_mask_flat) == len(example_args_flat), \"`nondiff_mask` must match `args`\"\n        nondiff_mask_flat = [(m or (not _is_floating(arg))) for m, arg in zip(nondiff_mask_flat, example_args_flat)]\n    else:\n        nondiff_mask_flat = [not _is_floating(arg) for i, arg in enumerate(example_args_flat)]\n\n    # define two torch helper functions for computing the VJP ######################################\n    def _torch_fn_diff_flat(*diff_args_flat, all_args_flat=None):\n        args_collected_flat, diff_args_flat = [], list(diff_args_flat)\n        for arg, m in zip(all_args_flat, nondiff_mask_flat):\n            args_collected_flat.append(arg if m else diff_args_flat.pop(0))\n        args_collected = tree_unflatten(args_struct, args_collected_flat)\n        return tree_flatten(torch_fn(*args_collected))[0]\n\n    # define the actual torch VJP function #########################################################\n    def bwd_fn_torch(args, gs):\n        args_flat = tree_flatten(args)[0]\n        diff_args_flat = [arg for (arg, m) in zip(args_flat, nondiff_mask_flat) if not m]\n        gs_flat = tree_flatten(gs)[0]\n\n        # use either torch's vjp or our custom vjp only wrt differentiable arguments ###############\n        grads_computed = False\n        if use_torch_vjp:\n            try:\n                diff_vjp_vals_flat = list(\n                    torch.func.vjp(partial(_torch_fn_diff_flat, all_args_flat=args_flat), *diff_args_flat)[1](gs_flat)\n                )\n                grads_computed = True\n            except RuntimeError:\n                tb = traceback.format_exc()\n                msg = (\n                    \"Somewhere in your PyTorch computation graph, a custom backward function is defined in the old way\"\n                    ' (see \"https://pytorch.org/docs/stable/notes/extending.html\"). This is only experimentally'\n                    \" supported in torch2jax. We will use a fallback based on `torch.autograd.grad` instead. Please\"\n                    \" pass `use_torch_vjp=False` to `torch2jax_with_vjp` if you wish to use this fallback explicitly.\"\n                    f\" Original error message:\\n{tb}\"\n                )\n                warn_once(msg, torch_fn)\n                grads_computed = False\n        if not grads_computed:\n            if not use_torch_vjp:\n                warn_once(\"You are NOT using PyTorch's functional VJP. This is highly experimental.\", torch_fn)\n            [diff_arg_flat.requires_grad_(True) for diff_arg_flat in diff_args_flat]\n            ret = sum(\n                torch.sum(g * r)\n                for (g, r) in zip(gs_flat, _torch_fn_diff_flat(*diff_args_flat, all_args_flat=args_flat))\n            )\n            diff_vjp_vals_flat = list(torch.autograd.grad(ret, diff_args_flat, create_graph=True))\n\n        # reconstruct the full vjp including for nondiff arguments #################################\n        vjp_vals_flat = []\n        for arg, m in zip(args_flat, nondiff_mask_flat):\n            vjp_vals_flat.append((None if not use_zeros else 0 * arg) if m else diff_vjp_vals_flat.pop(0))\n        return tree_unflatten(args_struct, vjp_vals_flat)\n\n    # construct example outputs out of the bwd_fn (sensitivty wrt args) ############################\n    # and next shapes (args, outputs) ##############################################################\n    example_outputs = normalize_shapes(output_shapes, example_args)\n    next_output_shapes = tree_unflatten(\n        args_struct,\n        [\n            ShapeDtypeStruct(dtype=dtype_t2j(x.dtype), shape=x.shape) if (not m or use_zeros) else None\n            for (x, m) in zip(example_args_flat, nondiff_mask_flat)\n        ],\n    )\n    bwd_fn = torch2jax_with_vjp(\n        bwd_fn_torch,\n        example_args,\n        example_outputs,\n        output_shapes=next_output_shapes,\n        depth=depth - 1,\n        use_torch_vjp=use_torch_vjp,\n        vmap_method=vmap_method,\n    )\n    # define the custom vjp using the fwd_fn and bwd_fn ############################################\n    fn.defvjp(fwd_fn, bwd_fn)\n\n    return fn\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#utilities","title":"<code>Utilities</code>","text":"<code>torch2jax.j2t(x, via='dlpack')</code> <p>Transfer a single jax.Array to a PyTorch tensor.</p> Source code in <code>torch2jax/dlpack_passing.py</code> <pre><code>def j2t(x: Array, via: str = \"dlpack\") -&gt; Tensor:\n    \"\"\"Transfer a single jax.Array to a PyTorch tensor.\"\"\"\n    try:\n        devices = x.devices()\n        if len(devices) &gt; 1:\n            msg = \"You are attempting to convert a JAX array with multiple devices to a PyTorch tensor.\"\n            msg += \" This is not supported\"\n            raise RuntimeError(msg)\n        device = list(devices)[0]\n    except ConcretizationTypeError:\n        msg = \"You are attempting to convert a non-concrete JAX array to a PyTorch tensor.\"\n        msg += \" This is not supported, since that JAX array does not contain any numbers.\"\n        raise RuntimeError(msg)\n    return _transfer(x, via=via, device=device)\n</code></pre> <code>torch2jax.t2j(x, via='dlpack')</code> <p>Transfer a single PyTorch tensor to a jax.Array.</p> Source code in <code>torch2jax/dlpack_passing.py</code> <pre><code>def t2j(x: Tensor, via: str = \"dlpack\") -&gt; Array:\n    \"\"\"Transfer a single PyTorch tensor to a jax.Array.\"\"\"\n    return _transfer(x, via=via, device=x.device)\n</code></pre> <code>torch2jax.tree_j2t(xs, via='dlpack')</code> <p>Transfer a tree of PyTorch tensors to a corresponding tree of jax.Array-s.</p> Source code in <code>torch2jax/dlpack_passing.py</code> <pre><code>def tree_j2t(xs: list[Array] | tuple[Array], via: str = \"dlpack\") -&gt; list[Tensor] | tuple[Tensor]:\n    \"\"\"Transfer a tree of PyTorch tensors to a corresponding tree of jax.Array-s.\"\"\"\n    return jax.tree.map(lambda x: j2t(x, via=via) if isinstance(x, Array) else x, xs)\n</code></pre> <code>torch2jax.tree_t2j(xs, via='dlpack')</code> <p>Transfer a tree of  jax.Array-s to a corresponding tree of PyTorch tensors.</p> Source code in <code>torch2jax/dlpack_passing.py</code> <pre><code>def tree_t2j(xs: list[Tensor] | tuple[Array], via: str = \"dlpack\") -&gt; list[Array] | tuple[Array]:\n    \"\"\"Transfer a tree of  jax.Array-s to a corresponding tree of PyTorch tensors.\"\"\"\n    return jax.tree.map(lambda x: t2j(x, via=via) if isinstance(x, Tensor) else x, xs)\n</code></pre>"},{"location":"examples/bert_example/","title":"Calling BERT model from JAX (with BERT weights in JAX)","text":"<pre><code>from __future__ import annotations\n\nimport sys\nfrom pathlib import Path\nimport random\nimport time\n\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel\nfrom datasets import load_dataset\nimport torch\nfrom torch import Tensor\nfrom torch.func import functional_call\nimport jax\nfrom jax import numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torch2jax import tree_t2j, torch2jax_with_vjp\n</code></pre>"},{"location":"examples/bert_example/#loading-the-dataset-and-the-model-in-pytorch","title":"Loading the dataset and the model (in PyTorch)","text":"<pre><code>dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\nmodel.to(device)\nmodel.eval()\n\n\ndef tokenizer_torch(text: list[str]) -&gt; dict[str, Tensor]:\n    encoded = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n    return {k: v.to(device) for (k, v) in encoded.items()}\n</code></pre>"},{"location":"examples/bert_example/#lets-convert-the-torch-model-to-a-function-using-torchfuncfunctional_call","title":"Let's convert the torch model to a function, using <code>torch.func.functional_call</code>","text":"<pre><code>params, buffers = dict(model.named_parameters()), dict(model.named_buffers())\n\ndef torch_fwd_fn(params, buffers, input):\n    return functional_call(model, (params, buffers), args=(), kwargs=input).pooler_output\n\nnb = 16\ntext = [x[\"text\"] for x in random.choices(dataset, k=int(1e3)) if len(x[\"text\"]) &gt; 100][:nb]\nencoded_text = tokenizer_torch(text)\n</code></pre>"},{"location":"examples/bert_example/#we-do-not-need-to-specify-output-the-library-will-call-the-torch-function-ones-to-infer-the-output","title":"We do not need to specify output, the library will call the torch function ones to infer the output","text":"<pre><code>jax_fwd_fn = jax.jit(torch2jax_with_vjp(torch_fwd_fn, params, buffers, encoded_text))\nparams_jax, buffers_jax = tree_t2j(params), tree_t2j(buffers)\nencoded_text_jax = tree_t2j(encoded_text)\n</code></pre>"},{"location":"examples/bert_example/#taking-gradients-wrt-model-parameters","title":"Taking gradients wrt model parameters","text":"<pre><code>g_fn = jax.jit(jax.grad(lambda params: jnp.sum(jax_fwd_fn(params, buffers_jax, encoded_text_jax))))\ng_torch_fn = torch.func.grad(lambda params: torch.sum(torch_fwd_fn(params, buffers, encoded_text)))\ngs = g_fn(params_jax)\ngs_torch = tree_t2j(g_torch_fn(params))\n\n# let's compare the errors in gradients (they will  be 0!)\ntotal_err = 0\nfor k in gs.keys():\n    err = jnp.linalg.norm(gs[k] - gs_torch[k])\n    total_err += err\nprint(f\"Total error in gradient: {total_err:.4e}\")\n</code></pre> <p>Total error in gradient: 0.0000e+00</p>"},{"location":"examples/bert_example/#timing-the-gains-over-pure_callback","title":"Timing the gains over <code>pure_callback</code>","text":"<pre><code>root_path = Path(\"\").absolute().parent / \"tests\"\nif str(root_path) not in sys.path:\n    sys.path.append(str(root_path))\n\nfrom pure_callback_alternative import wrap_torch_fn\n</code></pre> <pre><code>with torch.no_grad():\n    output_shapes = model(**encoded_text).pooler_output\njax_fwd_fn2 = jax.jit(wrap_torch_fn(torch_fwd_fn, output_shapes, device=device.type))\n</code></pre> <pre><code>t1s, t2s, t3s = [], [], []\nfor i in tqdm(range(100)):\n    text = [x[\"text\"] for x in random.choices(dataset, k=int(1e3)) if len(x[\"text\"]) &gt; 100][:nb]\n    encoded_text = tokenizer_torch(text)\n    encoded_text_jax = tree_t2j(encoded_text)\n\n    t = time.time()\n    out1 = jax_fwd_fn(params_jax, buffers_jax, encoded_text_jax)\n    out1.block_until_ready()\n    t = time.time() - t\n    t1s.append(t)\n\n    t = time.time()\n    out2 = jax_fwd_fn2(params_jax, buffers_jax, encoded_text_jax)[0]\n    out2.block_until_ready()\n    t = time.time() - t\n    t2s.append(t)\n\n    t = time.time()\n    with torch.no_grad():\n        out3 = model(**encoded_text).pooler_output\n    torch.cuda.synchronize()\n    t = time.time() - t\n    t3s.append(t)\n</code></pre> <pre><code>plt.figure()\nplt.bar(\n    [\"jax (pure_callback)\", \"jax (torch2jax)\", \"torch\"],\n    [np.mean(t2s), np.mean(t1s), np.mean(t3s)],\n    yerr=[np.std(t2s), np.std(t1s), np.std(t3s)],\n    capsize=10.0,\n)\nplt.ylabel(\"Time (s)\")\nplt.tight_layout()\nplt.savefig(\"../images/bert_from_jax.png\", dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\nplt.show()\n</code></pre>"},{"location":"examples/data_parallel/","title":"Data Parallel","text":"<pre><code>import functools\nimport copy\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport jax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as P, NamedSharding\nfrom torch2jax import torch2jax, torch2jax_with_vjp, tree_j2t, tree_t2j\n\n\ndef _setattr(mod, key, delim: str = \".\"):\n    if delim not in key:\n        setattr(mod, key, None)\n    else:\n        key, key_remaining = key.split(delim, 1)\n        _setattr(getattr(mod, key), key_remaining, delim=delim)\n\n\ndef _strip_model(model):\n    for key in dict(model.named_parameters()).keys():\n        _setattr(model, key, delim=\".\")\n\n\nif __name__ == \"__main__\":\n    model = nn.Sequential(nn.Linear(1024 * 1024, 1024), nn.SiLU(), nn.Linear(1024, 16)).to(\"cuda:0\")\n    params = dict(model.named_parameters())\n    [p.requires_grad_(False) for p in params.values()]\n    _strip_model(model)  # remove params from the model, leaving only a skeleton\n\n    def call_model_torch(x, params):\n        ys = []\n        for _ in range(30):\n            # functional_call uses the model in-place, we need a local copy\n            local_model_skeleton = copy.deepcopy(model)\n            ys.append(torch.func.functional_call(local_model_skeleton, params, x))\n        return sum(ys)\n\n    # jax init\n    devices = jax.devices(\"cuda\")\n    mesh = jax.make_mesh((len(devices),), P(\"x\"), devices=devices)\n    params_sharding = NamedSharding(mesh, P())  # fully replicated\n    batch_sharding = NamedSharding(mesh, P(\"x\", None))  # sharded along batch\n\n    x = jax.jit(\n        lambda: jax.random.normal(jax.random.key(0), (128, 1024 * 1024)),\n        out_shardings=batch_sharding,\n    )()\n\n    params = jax.tree.map(lambda p: jax.device_put(p, params_sharding), tree_t2j(params))\n    params_spec = jax.tree.map(lambda _: params_sharding.spec, params)\n\n    @jax.jit\n    @functools.partial(\n        shard_map,\n        mesh=mesh,\n        in_specs=(batch_sharding.spec, params_spec),\n        out_specs=batch_sharding.spec,\n        check_rep=False,\n    )\n    def fwd_fn(x, params):\n        return torch2jax_with_vjp(call_model_torch, x, params, output_shapes=x[:, :16])(x, params)\n\n    y = fwd_fn(x, params)\n\n    # OR using JIT (but without gradients)\n    fwd_fn = jax.jit(\n        torch2jax(\n            call_model_torch, x, params, output_shapes=x[:, :16], output_sharding_spec=P(\"x\", None)\n        )\n    )\n\n    y = fwd_fn(x, params)\n\n    # profile the computation\n    _ = fwd_fn(x, params)\n    path = Path(\"/tmp/profiles/data_parallel\")\n    path.mkdir(parents=True, exist_ok=True)\n    with jax.profiler.trace(str(path)):\n       for _ in range(10):\n           fwd_fn(x, params).block_until_ready()\n</code></pre>"},{"location":"examples/resnet_example/","title":"ResNet 50 example","text":"<pre><code>from __future__ import annotations\n\nfrom pprint import pprint\n\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nimport torch\nfrom torchvision.models.resnet import resnet18\nfrom torch import nn\nfrom torch.func import functional_call\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nimport jax\nfrom jax import numpy as jnp\nimport optax\n\nfrom torch2jax import tree_t2j, torch2jax_with_vjp, tree_j2t, t2j, j2t\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice_jax = jax.devices(device.type)[0]\n</code></pre>"},{"location":"examples/resnet_example/#loading-the-dataset-and-the-model-in-pytorch","title":"Loading the dataset and the model (in PyTorch)","text":"<pre><code>dataset = load_dataset(\"mnist\", split=\"train\")\n\ndef collate_torch_fn(batch):\n    imgs = torch.stack([ToTensor()(x[\"image\"]).repeat((3, 1, 1)) for x in batch]).to(device)\n    labels = torch.tensor([x[\"label\"] for x in batch]).to(device)\n    return imgs, labels\n\ncollate_jax_fn = lambda batch: tree_t2j(collate_torch_fn(batch))\n</code></pre> <pre><code>model = nn.Sequential(resnet18(), nn.Linear(1000, 10))\nmodel.to(device)\nmodel.eval()\n\nopts = dict(batch_size=32, shuffle=True, num_workers=0)\ndl = DataLoader(dataset, **opts)\ndl_jax = DataLoader(dataset, **dict(opts, collate_fn=collate_jax_fn))\ndl_torch = DataLoader(dataset, **dict(opts, collate_fn=collate_torch_fn))\n</code></pre>"},{"location":"examples/resnet_example/#lets-convert-the-torch-model-to-a-function-using-torchfuncfunctional_call","title":"Let's convert the torch model to a function, using <code>torch.func.functional_call</code>","text":"<pre><code>params, buffers = dict(model.named_parameters()), dict(model.named_buffers())\n\n\ndef torch_fwd_fn(params, buffers, input):\n    buffers = {k: torch.clone(v) for k, v in buffers.items()}\n    return functional_call(model, (params, buffers), args=input)\n\n\nXt, yt = next(iter(dl_torch))\nnondiff_argnums = (1, 2)  # buffers, input\njax_fwd_fn = jax.jit(\n    torch2jax_with_vjp(torch_fwd_fn, params, buffers, Xt, nondiff_argnums=nondiff_argnums)\n)\nparams_jax, buffers_jax = tree_t2j(params), tree_t2j(buffers)\n</code></pre>"},{"location":"examples/resnet_example/#lets-use-torchs-crossentropyloss","title":"Let's use torch's CrossEntropyLoss","text":"<pre><code>Xt, yt = next(iter(dl_torch))\ntorch_ce_fn = lambda yp, y: nn.CrossEntropyLoss()(yp, y)\njax_ce_fn = torch2jax_with_vjp(torch_ce_fn, model(Xt), yt)\n\njax_l_fn = jax.jit(\n    lambda params_jax, X, y: jnp.mean(jax_ce_fn(jax_fwd_fn(params_jax, buffers_jax, X), y))\n)\njax_g_fn = jax.jit(jax.grad(jax_l_fn))\ntorch_g_fn = torch.func.grad(\n    lambda params, Xt, yt: torch_ce_fn(torch_fwd_fn(params, buffers, Xt), yt)\n)\n</code></pre> <pre><code>X, y = next(iter(dl_jax))\ngs_jax = jax_g_fn(params_jax, X, y)\ngs_torch = torch_g_fn(params, *tree_j2t((X, y)))\n\n# let's compute error in gradients between JAX and Torch (the errors are 0!)\nerrors = {k: float(jnp.linalg.norm(v - t2j(gs_torch[k]))) for k, v in gs_jax.items()}\npprint(errors)\n</code></pre> <p> {'0.bn1.bias': 6.606649449736324e-09,  '0.bn1.weight': 1.0237145575686668e-09,  '0.conv1.weight': 1.9232666659263487e-07,  '0.fc.bias': 0.0,  '0.fc.weight': 0.0,  '0.layer1.0.bn1.bias': 4.424356436771859e-09,  '0.layer1.0.bn1.weight': 5.933196711715993e-10,  '0.layer1.0.bn2.bias': 2.3588471176339e-09,  '0.layer1.0.bn2.weight': 4.533372566228877e-10,  '0.layer1.0.conv1.weight': 1.4028480599392879e-08,  '0.layer1.0.conv2.weight': 1.1964990775936712e-08,  '0.layer1.1.bn1.bias': 8.75052974524948e-10,  '0.layer1.1.bn1.weight': 2.0072446482721773e-10,  '0.layer1.1.bn2.bias': 5.820766091346741e-11,  '0.layer1.1.bn2.weight': 2.9103830456733704e-11,  '0.layer1.1.conv1.weight': 1.1259264631746646e-08,  '0.layer1.1.conv2.weight': 1.1262083710050774e-08,  '0.layer2.0.bn1.bias': 0.0,  '0.layer2.0.bn1.weight': 0.0,  '0.layer2.0.bn2.bias': 0.0,  '0.layer2.0.bn2.weight': 0.0,  '0.layer2.0.conv1.weight': 0.0,  '0.layer2.0.conv2.weight': 0.0,  '0.layer2.0.downsample.0.weight': 6.819701248161891e-09,  '0.layer2.0.downsample.1.bias': 0.0,  '0.layer2.0.downsample.1.weight': 0.0,  '0.layer2.1.bn1.bias': 0.0,  '0.layer2.1.bn1.weight': 0.0,  '0.layer2.1.bn2.bias': 0.0,  '0.layer2.1.bn2.weight': 5.820766091346741e-11,  '0.layer2.1.conv1.weight': 0.0,  '0.layer2.1.conv2.weight': 0.0,  '0.layer3.0.bn1.bias': 0.0,  '0.layer3.0.bn1.weight': 0.0,  '0.layer3.0.bn2.bias': 0.0,  '0.layer3.0.bn2.weight': 0.0,  '0.layer3.0.conv1.weight': 0.0,  '0.layer3.0.conv2.weight': 0.0,  '0.layer3.0.downsample.0.weight': 0.0,  '0.layer3.0.downsample.1.bias': 0.0,  '0.layer3.0.downsample.1.weight': 0.0,  '0.layer3.1.bn1.bias': 0.0,  '0.layer3.1.bn1.weight': 0.0,  '0.layer3.1.bn2.bias': 0.0,  '0.layer3.1.bn2.weight': 0.0,  '0.layer3.1.conv1.weight': 0.0,  '0.layer3.1.conv2.weight': 0.0,  '0.layer4.0.bn1.bias': 0.0,  '0.layer4.0.bn1.weight': 0.0,  '0.layer4.0.bn2.bias': 0.0,  '0.layer4.0.bn2.weight': 0.0,  '0.layer4.0.conv1.weight': 0.0,  '0.layer4.0.conv2.weight': 0.0,  '0.layer4.0.downsample.0.weight': 0.0,  '0.layer4.0.downsample.1.bias': 0.0,  '0.layer4.0.downsample.1.weight': 0.0,  '0.layer4.1.bn1.bias': 0.0,  '0.layer4.1.bn1.weight': 0.0,  '0.layer4.1.bn2.bias': 0.0,  '0.layer4.1.bn2.weight': 0.0,  '0.layer4.1.conv1.weight': 0.0,  '0.layer4.1.conv2.weight': 0.0,  '1.bias': 0.0,  '1.weight': 0.0} </p>"},{"location":"examples/resnet_example/#train-loop","title":"Train loop","text":"<p>This isn't very efficient because torch synchronizes for every batch when called from JAX. Train in PyTorch, but you can do inference in JAX fast.</p> <pre><code>optimizer = optax.adam(1e-3)\nopt_state = optimizer.init(params_jax)\nupdate_fn, apply_updates = jax.jit(optimizer.update), jax.jit(optax.apply_updates)\nfor i, (X, y) in enumerate(tqdm(dl_jax, total=len(dl_jax))):\n    gs = jax_g_fn(params_jax, X, y)\n    updates, opt_state = update_fn(gs, opt_state)\n    params_jax2 = apply_updates(params_jax, updates)\n    if i &gt; 10:\n        break\n</code></pre>"}]}