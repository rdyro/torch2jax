{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bca5ac5",
   "metadata": {},
   "source": [
    "# `torch2jax` - demonstrating input/output shapes specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd89d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "\n",
    "from torch2jax import tree_t2j, torch2jax_with_vjp\n",
    "count_format = lambda i: {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[i] if i < 4 else f\"{i}th\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efd092",
   "metadata": {},
   "source": [
    "### Single output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9feffaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1.\n",
      "I get evaluated 1st time\n",
      "I get evaluated 2nd time\n",
      "[ 1.4628059  2.9096096 -0.5187819  4.7829256 -0.576247   2.7845862\n",
      "  1.3225893 -1.0953598 -1.8378775  1.493313 ]\n",
      "\n",
      "\n",
      "Case 2.\n",
      "I get evaluated 1st time\n",
      "[ 1.4628059  2.9096096 -0.5187819  4.7829256 -0.576247   2.7845862\n",
      "  1.3225893 -1.0953598 -1.8378775  1.493313 ]\n",
      "\n",
      "\n",
      "Case 3. (a)\n",
      "I get evaluated 1st time\n",
      "[ 1.4628059  2.9096096 -0.5187819  4.7829256 -0.576247   2.7845862\n",
      "  1.3225893 -1.0953598 -1.8378775  1.493313 ]\n",
      "\n",
      "\n",
      "Case 3. (b)\n",
      "I get evaluated 1st time\n",
      "[ 1.4628059  2.9096096 -0.5187819  4.7829256 -0.576247   2.7845862\n",
      "  1.3225893 -1.0953598 -1.8378775  1.493313 ]\n",
      "\n",
      "\n",
      "Case 3. (c)\n",
      "I get evaluated 1st time\n",
      "[ 1.4628059  2.9096096 -0.5187819  4.7829256 -0.576247   2.7845862\n",
      "  1.3225893 -1.0953598 -1.8378775  1.493313 ]\n"
     ]
    }
   ],
   "source": [
    "# single output example ############################################################################\n",
    "call_count = 0\n",
    "\n",
    "\n",
    "def fn(a, b):\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    print(f\"I get evaluated {count_format(call_count)} time\")\n",
    "    return a + b + torch.ones_like(a)\n",
    "\n",
    "\n",
    "a_torch, b_torch = torch.randn(10), torch.randn(10)\n",
    "a_jax, b_jax = tree_t2j((a_torch, b_torch))\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Case 1. we do not specify output_shapes\n",
    "# torch2jax will call the PyTorch function with example arguments and infer the output_shapes from \n",
    "# the output\n",
    "#\n",
    "# torch2jax DOES call the PyTorch function with example arguments\n",
    "print(\"Case 1.\")\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch)\n",
    "\n",
    "# this would not work because torch2jax will try to call the PyTorch function with jax arguments\n",
    "# jax_fn = torch2jax_with_vjp(fn, a_jax, b_jax) \n",
    "\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Case 2. we specify output_shapes, but not dtypes\n",
    "# torch2jax will guess that the dtype of all outputs is the same as floating point dtype of any of \n",
    "# the input\n",
    "#\n",
    "# torch2jax DOES NOT call the PyTorch function with example arguments\n",
    "output_shapes = torch.Size(a_jax.shape)\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch, output_shapes=output_shapes)\n",
    "print(\"\\n\\nCase 2.\")\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "####################################################################################################\n",
    "# Case 3. we specify output_shapes and dtypes\n",
    "# torch2jax does not have to guess anything, example arguments need only carry\n",
    "# information about shapes and dtypes\n",
    "#\n",
    "# torch2jax DOES NOT call the PyTorch function with example arguments\n",
    "\n",
    "# Case 3. (a) passing torch example arguments - works!\n",
    "print(\"\\n\\nCase 3. (a)\")\n",
    "output_shapes = jax.ShapeDtypeStruct(a_jax.shape, a_jax.dtype)\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "# Case 3. (b) passing jax example arguments - works! (because jax arguments have\n",
    "# both a dtype and a shape)\n",
    "print(\"\\n\\nCase 3. (b)\")\n",
    "jax_fn = torch2jax_with_vjp(fn, a_jax, b_jax, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "# Case 3. (c) passing shape and dtype structs - works! (because shape and dtype\n",
    "# structs have both a dtype and a shape)\n",
    "print(\"\\n\\nCase 3. (c)\")\n",
    "a_shape = jax.ShapeDtypeStruct(a_jax.shape, a_jax.dtype)\n",
    "b_shape = jax.ShapeDtypeStruct(b_jax.shape, b_jax.dtype)\n",
    "jax_fn = torch2jax_with_vjp(fn, a_shape, b_shape, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6766001",
   "metadata": {},
   "source": [
    "### Multiple output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0871b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1.\n",
      "I get evaluated 1st time\n",
      "I get evaluated 2nd time\n",
      "(Array([ 2.8916252 ,  0.8241199 ,  0.72123194,  0.98281103,  3.118568  ,\n",
      "        0.38162696,  1.3556712 ,  0.570555  , -0.6736982 ,  2.4688735 ],      dtype=float32), Array([-0.84350806, -0.50517464, -0.5478717 , -0.21732308,  1.1992476 ,\n",
      "        0.22803703, -0.4428231 ,  0.58957076, -0.9303674 , -1.063955  ],      dtype=float32))\n",
      "\n",
      "\n",
      "Case 2.\n",
      "I get evaluated 1st time\n",
      "(Array([ 2.8916252 ,  0.8241199 ,  0.72123194,  0.98281103,  3.118568  ,\n",
      "        0.38162696,  1.3556712 ,  0.570555  , -0.6736982 ,  2.4688735 ],      dtype=float32), Array([-0.84350806, -0.50517464, -0.5478717 , -0.21732308,  1.1992476 ,\n",
      "        0.22803703, -0.4428231 ,  0.58957076, -0.9303674 , -1.063955  ],      dtype=float32))\n",
      "\n",
      "\n",
      "Case 3. (a)\n",
      "I get evaluated 1st time\n",
      "(Array([ 2.8916252 ,  0.8241199 ,  0.72123194,  0.98281103,  3.118568  ,\n",
      "        0.38162696,  1.3556712 ,  0.570555  , -0.6736982 ,  2.4688735 ],      dtype=float32), Array([-0.84350806, -0.50517464, -0.5478717 , -0.21732308,  1.1992476 ,\n",
      "        0.22803703, -0.4428231 ,  0.58957076, -0.9303674 , -1.063955  ],      dtype=float32))\n",
      "\n",
      "\n",
      "Case 3. (b)\n",
      "I get evaluated 1st time\n",
      "(Array([ 2.8916252 ,  0.8241199 ,  0.72123194,  0.98281103,  3.118568  ,\n",
      "        0.38162696,  1.3556712 ,  0.570555  , -0.6736982 ,  2.4688735 ],      dtype=float32), Array([-0.84350806, -0.50517464, -0.5478717 , -0.21732308,  1.1992476 ,\n",
      "        0.22803703, -0.4428231 ,  0.58957076, -0.9303674 , -1.063955  ],      dtype=float32))\n",
      "\n",
      "\n",
      "Case 3. (c)\n",
      "I get evaluated 1st time\n",
      "(Array([ 2.8916252 ,  0.8241199 ,  0.72123194,  0.98281103,  3.118568  ,\n",
      "        0.38162696,  1.3556712 ,  0.570555  , -0.6736982 ,  2.4688735 ],      dtype=float32), Array([-0.84350806, -0.50517464, -0.5478717 , -0.21732308,  1.1992476 ,\n",
      "        0.22803703, -0.4428231 ,  0.58957076, -0.9303674 , -1.063955  ],      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# multiple outputs example #########################################################################\n",
    "call_count = 0\n",
    "\n",
    "\n",
    "def fn(a, b):\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    print(f\"I get evaluated {count_format(call_count)} time\")\n",
    "    return a + b + torch.ones_like(a), a - b\n",
    "\n",
    "\n",
    "a_torch, b_torch = torch.randn(10), torch.randn(10)\n",
    "a_jax, b_jax = tree_t2j((a_torch, b_torch))\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Case 1. we do not specify output_shapes\n",
    "# torch2jax will call the PyTorch function with example arguments and infer the output_shapes from\n",
    "# the output\n",
    "#\n",
    "# torch2jax DOES call the PyTorch function with example arguments\n",
    "print(\"Case 1.\")\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch)\n",
    "\n",
    "# this would not work because torch2jax will try to call the PyTorch function with jax arguments\n",
    "# jax_fn = torch2jax_with_vjp(fn, a_jax, b_jax) \n",
    "\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Case 2. we specify output_shapes, but not dtypes\n",
    "# torch2jax will guess that the dtype of all outputs is the same as floating point dtype of any of\n",
    "# the input\n",
    "#\n",
    "# torch2jax DOES NOT call the PyTorch function with example arguments\n",
    "output_shapes = (torch.Size(a_jax.shape), torch.Size(a_jax.shape))\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch, output_shapes=output_shapes)\n",
    "print(\"\\n\\nCase 2.\")\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "####################################################################################################\n",
    "# Case 3. we specify output_shapes and dtypes\n",
    "# torch2jax does not have to guess anything, example arguments need only carry\n",
    "# information about shapes and dtypes\n",
    "#\n",
    "# torch2jax DOES NOT call the PyTorch function with example arguments\n",
    "\n",
    "# Case 3. (a) passing torch example arguments - works!\n",
    "print(\"\\n\\nCase 3. (a)\")\n",
    "output_shapes = (\n",
    "    jax.ShapeDtypeStruct(a_jax.shape, a_jax.dtype),\n",
    "    jax.ShapeDtypeStruct(a_jax.shape, a_jax.dtype),\n",
    ")\n",
    "jax_fn = torch2jax_with_vjp(fn, a_torch, b_torch, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "# Case 3. (b) passing jax example arguments - works! (because jax arguments have\n",
    "# both a dtype and a shape)\n",
    "print(\"\\n\\nCase 3. (b)\")\n",
    "jax_fn = torch2jax_with_vjp(fn, a_jax, b_jax, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0\n",
    "\n",
    "# Case 3. (c) passing shape and dtype structs - works! (because shape and dtype\n",
    "# structs have both a dtype and a shape)\n",
    "print(\"\\n\\nCase 3. (c)\")\n",
    "a_shape = jax.ShapeDtypeStruct(a_jax.shape, a_jax.dtype)\n",
    "b_shape = jax.ShapeDtypeStruct(b_jax.shape, b_jax.dtype)\n",
    "jax_fn = torch2jax_with_vjp(fn, a_shape, b_shape, output_shapes=output_shapes)\n",
    "print(jax_fn(a_jax, b_jax))\n",
    "call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4568f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "devel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
