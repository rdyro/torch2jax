
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="The documentation for the torch2jax package.">
      
      
        <meta name="author" content="Robert Dyro">
      
      
        <link rel="canonical" href="https://rdyro.github.io/torch2jax/">
      
      
      
        <link rel="next" href="installation/">
      
      
      <link rel="icon" href="img/favicon.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.50">
    
    
      
        <title>torch2jax</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.a40c8224.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torch2jax" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="torch2jax" class="md-header__button md-logo" aria-label="torch2jax" data-md-component="logo">
      
  <img src="img/favicon.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            torch2jax
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overview
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/rdyro/torch2jax/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    rdyro/torch2jax
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="torch2jax" class="md-nav__button md-logo" aria-label="torch2jax" data-md-component="logo">
      
  <img src="img/favicon.svg" alt="logo">

    </a>
    torch2jax
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/rdyro/torch2jax/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    rdyro/torch2jax
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="roadmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Roadmap
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Changelog
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="examples/bert_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="examples/resnet_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ResNet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="examples/data_parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Parallel
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Full API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Full API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="api/torch2jax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch2jax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="api/torch2jax_with_vjp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch2jax_with_vjp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="api/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="torch2jax">torch2jax</h1>
<p><a href="https://rdyro.github.io/torch2jax/">Documentation</a></p>
<p><a href="https://rdyro.github.io/torch2jax/"></p>
<p align="center">
<img src="img/torch2jax_logo2.png" style="max-width:800px;width:70%;display:block;margin-left:auto;margin-right:auto"/>
</p>
<p></a>
<br /></p>
<p>This package is designed to facilitate no-copy PyTorch calling from JAX under
both eager execution and JIT. It leverages the JAX C++ extension interface,
enabling operations on both CPU and GPU platforms. Moreover, it allows for
executing arbitrary PyTorch code from JAX under eager execution and JIT.</p>
<p>The intended application is efficiently running existing PyTorch code (like ML
models) in JAX applications with very low overhead.</p>
<p>This project was inspired by the jax2torch repository
<a href="https://github.com/lucidrains/jax2torch">https://github.com/lucidrains/jax2torch</a>
and has been made possible due to an amazing tutorial on extending JAX
<a href="https://github.com/dfm/extending-jax">https://github.com/dfm/extending-jax</a>.
Comprehensive JAX documentation
<a href="https://github.com/google/jax">https://github.com/google/jax</a> also
significantly contributed to this work.</p>
<p>Although I am unsure this functionality could be achieved without C++/CUDA, the
C++ compilation is efficiently done using PyTorch's portable CUDA &amp; C++
compilation features, requiring minimal configuration.</p>
<h1 id="install">Install</h1>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/rdyro/torch2jax.git
</code></pre></div>
<p><code>torch2jax</code> is now available on PyPI under the alias <code>wrap_torch2jax</code>:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>wrap-torch2jax
$<span class="w"> </span><span class="c1"># then</span>
$<span class="w"> </span>python3
$<span class="w"> </span>&gt;&gt;&gt;<span class="w"> </span>from<span class="w"> </span>wrap_torch2jax<span class="w"> </span>import<span class="w"> </span>torch2jax,<span class="w"> </span>torch2jax_with_vjp
</code></pre></div>
<p>Tested on:
  - CPU: Python: <code>3.9 3.10 3.11 3.12</code> &amp; JAX Versions <code>0.4.26 0.4.27 0.4.28 0.4.29 0.4.30 0.4.31</code>
  - CUDA: Python <code>3.9 3.10 3.11 3.12</code> &amp; JAX Versions <code>0.4.30 0.4.31</code></p>
<h1 id="usage">Usage</h1>
<p>With a single output</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="nn">torch2jax</span> <span class="kn">import</span> <span class="n">torch2jax</span>  <span class="c1"># this converts a Python function to JAX</span>
<span class="kn">from</span> <span class="nn">torch2jax</span> <span class="kn">import</span> <span class="n">Size</span><span class="p">,</span> <span class="n">dtype_t2j</span>  <span class="c1"># this is torch.Size, a tuple-like shape representation</span>


<span class="k">def</span> <span class="nf">torch_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>


<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">jax_fn</span> <span class="o">=</span> <span class="n">torch2jax</span><span class="p">(</span><span class="n">torch_fn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># without output_shapes, torch_fn **will be evaluated once**</span>
<span class="n">jax_fn</span> <span class="o">=</span> <span class="n">torch2jax</span><span class="p">(</span><span class="n">torch_fn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">output_shapes</span><span class="o">=</span><span class="n">Size</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>  <span class="c1"># torch_fn will NOT be evaluated</span>

<span class="c1"># you can specify the whole input and output structure without instantiating the tensors</span>
<span class="c1"># torch_fn will NOT be evaluated</span>
<span class="n">jax_fn</span> <span class="o">=</span> <span class="n">torch2jax</span><span class="p">(</span>
    <span class="n">torch_fn</span><span class="p">,</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
    <span class="n">output_shapes</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
<span class="p">)</span>

<span class="n">prngkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># both CPU and CUDA are supported</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">prngkey</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">prngkey</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>

<span class="c1"># call the no-copy torch function</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">jax_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># call the no-copy torch function **under JIT**</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax_fn</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div>
<p>With a multiple outputs</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">torch_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">layer</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>


<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">jax_fn</span> <span class="o">=</span> <span class="n">torch2jax</span><span class="p">(</span><span class="n">torch_fn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># with example argumetns</span>

<span class="n">prngkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">prngkey</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">prngkey</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>

<span class="c1"># call the no-copy torch function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">jax_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># call the no-copy torch function **under JIT**</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax_fn</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div>
<p>For a more advanced discussion on different ways of specifying input/output
specification of the wrapped function, take a look at:
<a href="examples/input_output_specification.ipynb">input_output_specification.ipynb</a>
notebook in the <code>examples</code> folder.</p>
<h1 id="automatically-defining-gradients">Automatically defining gradients</h1>
<p>Automatic reverse-mode gradient definitions are now supported for wrapped
pytorch functions with the method <code>torch2jax_with_vjp</code></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">torch2jax</span> <span class="kn">import</span> <span class="n">torch2jax_with_vjp</span>

<span class="k">def</span> <span class="nf">torch_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,)</span>

<span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># `depth` determines how many times the function can be differentiated</span>
<span class="n">jax_fn</span> <span class="o">=</span> <span class="n">torch2jax_with_vjp</span><span class="p">(</span><span class="n">torch_fn</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 


<span class="c1"># we can now differentiate the function (derivatives are taken using PyTorch autodiff)</span>
<span class="n">g_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">jax_fn</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">g_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># JIT works too</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">g_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</code></pre></div>
<p>Caveats: </p>
<ul>
<li><code>jax.hessian(f)</code> will not work since <code>torch2jax</code> uses forward differentiation, but
  the same functionality can be achieved using <code>jax.jacobian(jax.jacobian(f))</code></li>
<li>input shapes are fixed for one wrapped function and cannot change, use
  <code>torch2jax_with_vjp/torch2jax</code> again if you need to alter the input shapes</li>
<li>in line with JAX philosphy, PyTorch functions must be non-mutable,
  <a href="https://pytorch.org/docs/master/func.html">torch.func</a> has a good description
  of how to convert e.g., PyTorch models, to non-mutable formulation</li>
</ul>
<h1 id="new-performant-multi-gpu-experimental">NEW: Performant multi-gpu (experimental)</h1>
<p>User feedback greatly appreciated, feel free to open an issue if you have any
questions or are running into issues: <a href="https://github.com/rdyro/torch2jax/issues/new">https://github.com/rdyro/torch2jax/issues/new</a>.</p>
<p><code>torch2jax</code> should now support efficient multi-gpu calling. JAX, broadly,
provides 3 main ways of calling multi-device code:
  - <code>shard_map</code> - per-device view, the <strong>recommended</strong> way of using <code>torch2jax</code>
    - supports sharded multi-GPU arrays called in parallel
    - supports automatically defining gradients
  - <code>jax.jit</code> - the jit function supports automatic computation on sharded
  inputs; this is currently partially supported by providing the sharding spec 
  of the output
  - <code>jax.pmap</code> - (DOES NOT WORK) works somewhat like <code>shard_map</code>, but with
  <code>jnp.stack</code> instead of <code>jnp.concatenate</code> over devices, please use <code>shard_map</code>
  instead</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Data-parallel model</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span>
<span class="kn">from</span> <span class="nn">jax.sharding</span> <span class="kn">import</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span><span class="p">,</span> <span class="n">NamedSharding</span>
<span class="kn">from</span> <span class="nn">torch2jax</span> <span class="kn">import</span> <span class="n">torch2jax</span><span class="p">,</span> <span class="n">torch2jax_with_vjp</span><span class="p">,</span> <span class="n">tree_t2j</span>


<span class="k">def</span> <span class="nf">_setattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">delim</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">delim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">key_remaining</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">delim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_setattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span> <span class="n">key_remaining</span><span class="p">,</span> <span class="n">delim</span><span class="o">=</span><span class="n">delim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_strip_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">_setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">delim</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
    <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
    <span class="n">_strip_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># remove params from the model, leaving only a skeleton</span>

    <span class="k">def</span> <span class="nf">call_model_torch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
            <span class="c1"># functional_call uses the model in-place, we need a local copy</span>
            <span class="n">local_model_skeleton</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">local_model_skeleton</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

    <span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">make_mesh</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),),</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span> <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">)</span>
    <span class="n">params_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">())</span>  <span class="c1"># fully replicated</span>
    <span class="n">batch_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>  <span class="c1"># sharded along batch</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)),</span>
        <span class="n">out_shardings</span><span class="o">=</span><span class="n">batch_sharding</span><span class="p">,</span>
    <span class="p">)()</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">params_sharding</span><span class="p">),</span> <span class="n">tree_t2j</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="n">params_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">params_sharding</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="nd">@functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">shard_map</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">batch_sharding</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">),</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">batch_sharding</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span>
        <span class="n">check_rep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">fwd_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch2jax_with_vjp</span><span class="p">(</span><span class="n">call_model_torch</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">output_shapes</span><span class="o">=</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">16</span><span class="p">])(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">fwd_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># OR using JIT (but without gradients)</span>
    <span class="n">fwd_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
        <span class="n">torch2jax</span><span class="p">(</span>
            <span class="n">call_model_torch</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">output_shapes</span><span class="o">=</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">16</span><span class="p">],</span> <span class="n">output_sharding_spec</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">fwd_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <img src="img/data_parallel.png" style="width: 100%; max-width: 1000px; height: auto; max-height: 1000px;">
  <p align="center">Fig: Overlapping torch calls on multiple devices (RTX A4000 x 4)</p>
</p>

<blockquote>
<p>Note: <code>jax.vmap</code>'s semantics might indicate that it can compute on sharded
arrays, it can work, but it is not recommend, and because of <code>torch2jax</code>'s
implementation will likely be executed sequentially (and likely be slow).</p>
</blockquote>
<h1 id="dealing-with-changing-shapes">Dealing with Changing Shapes</h1>
<p>You can deal with changing input shapes by calling <code>torch2jax</code> (and
<code>torch2jax_with_vjp</code>) in the JAX function, both under JIT and eagerly!</p>
<div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch2jax_with_vjp</span><span class="p">(</span>
        <span class="n">torch_fn</span><span class="p">,</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
        <span class="n">output_shapes</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype_t2j</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span>
    <span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span> <span class="o">-</span> <span class="n">c</span>

<span class="nb">print</span><span class="p">(</span><span class="n">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
</code></pre></div>
<h1 id="timing-comparison-vs-pure_callback">Timing Comparison vs <code>pure_callback</code></h1>
<p>This package achieves a much better performance when calling PyTorch code from
JAX because it does not copy its input arguments and does not move CUDA data off
the GPU.</p>
<p><img src="img/time_difference.png"></p>
<h1 id="current-limitations-of-torch2jax">Current Limitations of <code>torch2jax</code></h1>
<ul>
<li>compilation happens on module import and can take 1-2 minutes (it will be cached afterwards)</li>
<li>in the Pytorch function all arguments must be tensors, all outputs must be tensors</li>
<li>all arguments must be on the same device and of the same datatype, either float32 or float64</li>
<li>an input/output shape (e.g. <code>output_shapes=</code> kw argument) representations (for
  flexibility in input and output structure) must be wrapped in <code>torch.Size</code> or
  <code>jax.ShapeDtypeStruct</code></li>
<li>the current implementation does not support batching, that's on the roadmap</li>
<li>the current implementation does not define the VJP rule, in current design, this has to be done in 
  Python</li>
</ul>
<h1 id="changelog">Changelog</h1>
<ul>
<li>version 0.6.0<ul>
<li>proper multi-GPU support mostly with <code>shard_map</code> but also via <code>jax.jit</code> automatic sharding</li>
<li><code>shard_map</code> and automatic <code>jax.jit</code> device parallelization should work, but <code>pmap</code> doesn't work</li>
<li>removed (deprecated)<ul>
<li>torch2jax_flat - use the more flexible torch2jax</li>
</ul>
</li>
<li>added input shapes validation - routines</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.5.0<ul>
<li>updating to the new JAX ffi interface</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.11<ul>
<li>compilation fixes and support for newer JAX versions</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.10<ul>
<li>support for multiple GPUs, currently, all arguments must and the output
  must be on the same GPU (but you can call the wrapped function with
  different GPUs in separate calls)</li>
<li>fixed the coming depreciation in JAX deprecating <code>.device()</code> for
  <code>.devices()</code></li>
</ul>
</li>
</ul>
<ul>
<li>no version change<ul>
<li>added helper script <code>install_package_aliased.py</code> to automatically install
  the package with a different name (to avoid a name conflict)</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.7<ul>
<li>support for newest JAX (0.4.17) with backwards compatibility maintained</li>
<li>compilation now delegated to python version subfolders for multi-python systems</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.6<ul>
<li>bug-fix: cuda stream is now synchronized before and after a torch call explicitly to
  avoid reading unwritten data</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.5<ul>
<li><code>torch2jax_with_vjp</code> now automatically selects <code>use_torch_vjp=False</code> if the <code>True</code> fails</li>
<li>bug-fix: cuda stream is now synchronized after a torch call explicitly to
  avoid reading unwritten data</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.4<ul>
<li>introduced a <code>use_torch_vjp</code> (defaulting to True) flag in <code>torch2jax_with_vjp</code> which 
  can be set to False to use the old <code>torch.autograd.grad</code> for taking
  gradients, it is the slower method, but is more compatible</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.3<ul>
<li>added a note in README about specifying input/output structure without instantiating data</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.2<ul>
<li>added <code>examples/input_output_specification.ipynb</code> showing how input/output
structure can be specified</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.1<ul>
<li>bug-fix: in <code>torch2jax_with_vjp</code>, nondiff arguments were erroneously memorized</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.4.0<ul>
<li>added batching (vmap support) using <code>torch.vmap</code>, this makes <code>jax.jacobian</code> work</li>
<li>robustified support for gradients</li>
<li>added mixed type arguments, including support for float16, float32, float64 and integer types</li>
<li>removed unnecessary torch function calls in defining gradients</li>
<li>added an example of wrapping a BERT model in JAX (with weights modified from JAX), <code>examples/bert_from_jax.ipynb</code></li>
</ul>
</li>
</ul>
<ul>
<li>version 0.3.0<ul>
<li>added a beta-version of a new wrapping method <code>torch2jax_with_vjp</code> which
allows recursively defining reverse-mode gradients for the wrapped torch
function that works in JAX both normally and under JIT</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.2.0<ul>
<li>arbitrary input and output structure is now allowed</li>
<li>removed the restriction on the number of arguments or their maximum dimension</li>
<li>old interface is available via <code>torch2jax.compat.torch2jax</code></li>
</ul>
</li>
</ul>
<ul>
<li>version 0.1.2<ul>
<li>full CPU only version support, selected via <code>torch.cuda.is_available()</code></li>
<li>bug-fix: compilation should now cache properly</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.1.1<ul>
<li>bug-fix: functions do not get overwritten, manual fn id parameter replaced with automatic id generation</li>
<li>compilation caching is now better</li>
</ul>
</li>
</ul>
<ul>
<li>version 0.1.0<ul>
<li>first working version of the package</li>
</ul>
</li>
</ul>
<h1 id="roadmap">Roadmap</h1>
<ul class="task-list">
<li class="task-list-item"><input type="checkbox" disabled checked/> call PyTorch functions on JAX data without input data copy</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> call PyTorch functions on JAX data without input data copy under jit</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> support both GPU and CPU</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (feature) support partial CPU building on systems without CUDA</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (user-friendly) support functions with a single output (return a single output, not a tuple)</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (user-friendly) support arbitrary argument input and output structure (use pytrees on the 
      Python side)</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (feature) support batching (e.g., support for <code>jax.vmap</code>)</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (feature) support integer input/output types</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (feature) support mixed-precision arguments in inputs/outputs</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (feature) support defining VJP for the wrapped function (import the experimental functionality 
      from <a href="https://github.com/rdyro/jfi-JAXFriendlyInterface">jit-JAXFriendlyInterface</a>)</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (tests) test how well device mapping works on multiple GPUs</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> (tests) setup automatic tests for multiple versions of Python, PyTorch and JAX</li>
<li class="task-list-item"><input type="checkbox" disabled/> (feature) look into supporting in-place functions (support for output without copy)</li>
<li class="task-list-item"><input type="checkbox" disabled/> (feature) support TPU</li>
</ul>
<h1 id="related-work">Related Work</h1>
<p>Our Python package wraps PyTorch code as-is (so custom code and mutating code
will work!), but if you're looking for an automatic way to transcribe a
supported subset of PyTorch code to JAX, take a look at
<a href="https://github.com/samuela/torch2jax/tree/main">https://github.com/samuela/torch2jax/tree/main</a>.</p>
<p>We realize that two packages named the same is not ideal. As we work towards a
solution, here's a stop-gap solution. We offer a helper script to install the
package with an alias name, installing our package using pip under a different
name.</p>
<ol>
<li><code>$ git clone https://github.com/rdyro/torch2jax.git</code> - clone this repo</li>
<li><code>$ python3 install_package_aliased.py new_name_torch2jax --install --test</code> - install and test this package under the name <code>new_name_torch2jax</code></li>
<li>you can now use this package under the name <code>new_name_torch2jax</code></li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.sections", "toc.integrate", "search.suggest", "search.highlight"], "search": "assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.60a45f97.min.js"></script>
      
    
  </body>
</html>